{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dc23c1",
   "metadata": {},
   "source": [
    "# ðŸ¤– Application 2: Logistic Regression for Classification\n",
    "\n",
    "> *\"Logistic regression is the gateway from regression to the world of classification.\"*\n",
    "\n",
    "In our second application, we'll tackle a **classification** problem. Instead of predicting a continuous value (like a price), we want to predict a discrete category (e.g., Yes/No, Spam/Not Spam). We'll build **Logistic Regression**, a fundamental classification algorithm, from scratch.\n",
    "\n",
    "## ðŸŽ¯ How the Math Comes Together\n",
    "\n",
    "- **Linear Algebra**: We still start with a linear equation, `z = X @ theta`, just like in linear regression.\n",
    "- **Calculus & Probability**: We then pass the result `z` through a special function called the **Sigmoid (or Logistic) function**. This function squashes any input value into a range between 0 and 1, which we can interpret as a probability. The loss function we use, **Log Loss (or Binary Cross-Entropy)**, is derived from the principle of maximum likelihood estimation.\n",
    "- **Optimization**: We'll again use Gradient Descent to find the parameters `theta` that minimize the Log Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9eb005",
   "metadata": {},
   "source": [
    "## ðŸ“š Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Load a synthetic dataset for classification\n",
    "iris = datasets.load_iris()\n",
    "# We'll use only 2 features and 2 classes for easy visualization\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(int)  # 1 if Iris-Virginica, else 0\n",
    "\n",
    "print(\"ðŸ¤– Libraries and Iris dataset loaded!\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[y==0][:, 0], X[y==0][:, 1], color='blue', label='Not Iris-Virginica')\n",
    "plt.scatter(X[y==1][:, 0], X[y==1][:, 1], color='green', label='Iris-Virginica')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('Classification Problem: Iris Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79afc50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“ Step 1: The Sigmoid Function and Model\n",
    "\n",
    "### The Sigmoid Function (Calculus & Probability)\n",
    "The sigmoid function is the heart of logistic regression. It takes any real number and maps it to a probability between 0 and 1.\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "### The Logistic Regression Model\n",
    "1. First, we compute a linear combination of the inputs (just like linear regression): `z = X_b @ Î¸`\n",
    "2. Then, we apply the sigmoid function to get the predicted probability: `p_hat = Ïƒ(z)`\n",
    "3. Finally, we make a class prediction based on this probability (e.g., if `p_hat >= 0.5`, predict class 1, else predict class 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Visualize the sigmoid function\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.plot(z, sigmoid(z), 'b-')\n",
    "plt.xlabel('z (Linear Combination)')\n",
    "plt.ylabel('Ïƒ(z) (Probability)')\n",
    "plt.title('The Sigmoid Function', fontsize=16, weight='bold')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Decision Boundary (0.5)')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.5)\n",
    "plt.axhline(y=1, color='k', linestyle='-', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e743c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“‰ Step 2: The Loss Function and Gradient (Calculus)\n",
    "\n",
    "We can't use MSE for classification. Instead, we use the **Log Loss** (or Binary Cross-Entropy) function. This function penalizes the model heavily when it makes a confident but incorrect prediction.\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)})] $$\n",
    "\n",
    "The gradient of this loss function with respect to the parameters `Î¸` is surprisingly simple and looks very similar to the one for linear regression:\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\frac{1}{m} X_b^T \\cdot (\\sigma(X_b \\cdot \\theta) - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc020ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# âš™ï¸ Step 3: The Training (Optimization)\n",
    "\n",
    "The training process is identical to linear regression, but we use the sigmoid function in our prediction and the log loss gradient for our updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c721af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X, y, learning_rate=0.1, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression model using Batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    # Add bias term to X\n",
    "    X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "    m, n = X_b.shape\n",
    "    y = y.reshape(-1, 1) # Ensure y is a column vector\n",
    "    \n",
    "    # 1. Initialize parameters\n",
    "    theta = np.random.randn(n, 1)\n",
    "    \n",
    "    # Keep track of loss\n",
    "    loss_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Calculate predictions (probabilities)\n",
    "        z = X_b.dot(theta)\n",
    "        p_hat = sigmoid(z)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        gradients = 1/m * X_b.T.dot(p_hat - y)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        # Calculate and store loss (with a small epsilon to avoid log(0))\n",
    "        epsilon = 1e-7\n",
    "        loss = -np.mean(y * np.log(p_hat + epsilon) + (1 - y) * np.log(1 - p_hat + epsilon))\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "    return theta, loss_history\n",
    "\n",
    "# Train the model\n",
    "final_theta, loss_history = train_logistic_regression(X, y, learning_rate=0.1, n_iterations=3000)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "print(f\"Final learned parameters (theta):\\n  Î¸â‚€ (bias): {final_theta[0][0]:.4f}\\n  Î¸â‚ (petal length): {final_theta[1][0]:.4f}\\n  Î¸â‚‚ (petal width): {final_theta[2][0]:.4f}\")\n",
    "print(f\"Initial Loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final Loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f357c55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Step 4: Visualization and Analysis\n",
    "\n",
    "The result of logistic regression is not a line, but a **decision boundary**. This is the line where the predicted probability is exactly 0.5. On one side, the model predicts class 1, and on the other, it predicts class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(theta, X, y, loss_history):\n",
    "    \"\"\"\n",
    "    Visualize the decision boundary and the training loss.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Plot the loss curve\n",
    "    ax1.plot(loss_history)\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Log Loss')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=16, weight='bold')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Plot the decision boundary\n",
    "    ax2.scatter(X[y==0][:, 0], X[y==0][:, 1], color='blue', label='Not Iris-Virginica')\n",
    "    ax2.scatter(X[y==1][:, 0], X[y==1][:, 1], color='green', label='Iris-Virginica')\n",
    "    \n",
    "    # Create a mesh to plot the boundary\n",
    "    x0, x1 = np.meshgrid(\n",
    "        np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 500).reshape(-1, 1),\n",
    "        np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 200).reshape(-1, 1)\n",
    "    )\n",
    "    X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "    X_new_b = np.c_[np.ones((len(X_new), 1)), X_new]\n",
    "    \n",
    "    # Get probabilities for the mesh\n",
    "    y_proba = sigmoid(X_new_b.dot(theta))\n",
    "    zz = y_proba.reshape(x0.shape)\n",
    "    \n",
    "    # Plot the contour (the decision boundary is the 0.5 level)\n",
    "    contour = ax2.contour(x0, x1, zz, cmap=plt.cm.brg, levels=[0.5])\n",
    "    ax2.clabel(contour, inline=True, fontsize=12)\n",
    "    \n",
    "    ax2.set_xlabel('Petal Length (cm)')\n",
    "    ax2.set_ylabel('Petal Width (cm)')\n",
    "    ax2.set_title('Logistic Regression Decision Boundary', fontsize=16, weight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_decision_boundary(final_theta, X, y, loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a80a9",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- **Loss Curve (Left)**: As with linear regression, the log loss decreases steadily, showing that our model is learning to distinguish between the two classes more accurately over time.\n",
    "- **Decision Boundary (Right)**: The plot shows the final linear decision boundary that the model has learned. Any new flower that falls to the upper-right of this line will be classified as Iris-Virginica (probability > 0.5), and any flower to the lower-left will be classified as not Iris-Virginica (probability < 0.5).\n",
    "\n",
    "This application demonstrates how a simple modificationâ€”adding a sigmoid function and changing the loss functionâ€”allows us to adapt the machinery of linear regression to solve classification problems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
