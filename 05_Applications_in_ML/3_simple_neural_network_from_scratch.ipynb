{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b22f39",
   "metadata": {},
   "source": [
    "# ü§ñ Application 3: A Simple Neural Network for Image Classification\n",
    "\n",
    "> *\"A neural network is just a stack of linear algebra and a sprinkle of calculus.\"*\n",
    "\n",
    "This is our capstone application! We will build a simple, one-hidden-layer neural network from scratch to classify handwritten digits from the famous MNIST dataset. This will bring together all the concepts from this course in a single, powerful example.\n",
    "\n",
    "## üéØ How the Math Comes Together\n",
    "\n",
    "- **Linear Algebra**: The core of the network! Each layer performs a matrix multiplication (`W @ X + b`) to transform the data. We will be working with matrices of weights and vectors of inputs and biases.\n",
    "- **Calculus**: We'll use the **Chain Rule** to perform **Backpropagation**. This is the algorithm for calculating the gradients of the loss function with respect to every weight and bias in the network, even those in the hidden layers. We'll also use an activation function (like ReLU or Tanh) and its derivative.\n",
    "- **Probability & Statistics**: The final layer will use a **Softmax function** to output a probability distribution over the 10 possible digit classes. The loss function we'll use, **Cross-Entropy Loss**, is the multi-class generalization of the Log Loss we saw in logistic regression.\n",
    "- **Optimization**: We'll use Mini-Batch Gradient Descent with an advanced optimizer like Adam to train the network's weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2697b7",
   "metadata": {},
   "source": [
    "## üìö Import Essential Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Loading MNIST dataset...\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X = X / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {np.argmax(y_train[i])}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cb4ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Step 1: Building the Network Architecture\n",
    "\n",
    "Our network will have:\n",
    "1. **An Input Layer**: 784 neurons (one for each pixel in a 28x28 image).\n",
    "2. **A Hidden Layer**: We'll choose 128 neurons. This layer will use the Tanh activation function.\n",
    "3. **An Output Layer**: 10 neurons (one for each digit, 0-9). This layer will use the Softmax activation function to produce probabilities.\n",
    "\n",
    "### Activation Functions (Calculus)\n",
    "- **Tanh (Hyperbolic Tangent)**: Squashes values to a range of [-1, 1]. It's a zero-centered alternative to the sigmoid.\n",
    "- **Softmax**: Generalizes the sigmoid function for multi-class problems. It takes a vector of scores and turns it into a probability distribution where all probabilities sum to 1.\n",
    "$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def softmax(z):\n",
    "    # Subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\" Initialize weights and biases for the network. \"\"\"\n",
    "    # Use He initialization for weights\n",
    "    W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a972d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Step 2: Forward and Backward Propagation\n",
    "\n",
    "### Forward Propagation (Linear Algebra)\n",
    "This is how we make a prediction. Data flows forward through the network.\n",
    "1. `Z1 = X @ W1 + b1`\n",
    "2. `A1 = tanh(Z1)` (Activation for hidden layer)\n",
    "3. `Z2 = A1 @ W2 + b2`\n",
    "4. `A2 = softmax(Z2)` (Final probabilities)\n",
    "\n",
    "### Backward Propagation (Calculus - The Chain Rule)\n",
    "This is how we learn. We calculate the error at the output and propagate it backward through the network to find out how much each weight and bias contributed to the error. This gives us the gradients.\n",
    "\n",
    "1. **Output Layer Error (`dZ2`)**: `A2 - y`\n",
    "2. **Output Layer Gradients (`dW2`, `db2`)**: `dW2 = A1.T @ dZ2`, `db2 = sum(dZ2)`\n",
    "3. **Hidden Layer Error (`dZ1`)**: `dZ2 @ W2.T * tanh_derivative(Z1)`\n",
    "4. **Hidden Layer Gradients (`dW1`, `db1`)**: `dW1 = X.T @ dZ1`, `db1 = sum(dZ1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = X @ params['W1'] + params['b1']\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = A1 @ params['W2'] + params['b2']\n",
    "    A2 = softmax(Z2)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def backward_propagation(X, y, params, cache):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer\n",
    "    dZ2 = cache['A2'] - y\n",
    "    dW2 = (1/m) * cache['A1'].T @ dZ2\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Hidden layer\n",
    "    dA1 = dZ2 @ params['W2'].T\n",
    "    dZ1 = dA1 * tanh_derivative(cache['Z1'])\n",
    "    dW1 = (1/m) * X.T @ dZ1\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca6364",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è Step 3: The Training Loop (Optimization)\n",
    "\n",
    "Now we put it all together in a training loop using Mini-Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, y_train, hidden_size=128, epochs=10, batch_size=64, learning_rate=0.1):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    \n",
    "    params = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the data\n",
    "        permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled = X_train[permutation]\n",
    "        y_shuffled = y_train[permutation]\n",
    "        \n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Forward propagation\n",
    "            A2, cache = forward_propagation(X_batch, params)\n",
    "            \n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(X_batch, y_batch, params, cache)\n",
    "            \n",
    "            # Update parameters (simple GD)\n",
    "            params['W1'] -= learning_rate * grads['dW1']\n",
    "            params['b1'] -= learning_rate * grads['db1']\n",
    "            params['W2'] -= learning_rate * grads['dW2']\n",
    "            params['b2'] -= learning_rate * grads['db2']\n",
    "        \n",
    "        # Calculate loss for the epoch\n",
    "        full_preds, _ = forward_propagation(X_train, params)\n",
    "        # Cross-entropy loss\n",
    "        loss = -np.mean(y_train * np.log(full_preds + 1e-8))\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
    "        \n",
    "    return params, loss_history\n",
    "\n",
    "# Train the neural network\n",
    "trained_params, loss_history = train_nn(X_train, y_train, epochs=15, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d768a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Step 4: Evaluation and Analysis\n",
    "\n",
    "Let's see how well our trained network performs on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b188a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test, y_test, params):\n",
    "    \"\"\" Evaluate the model's accuracy on the test set. \"\"\"\n",
    "    # Make predictions\n",
    "    predictions, _ = forward_propagation(X_test, params)\n",
    "    \n",
    "    # Get the class with the highest probability\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(X_test, y_test, trained_params)\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss Curve', fontsize=16, weight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "predictions, _ = forward_propagation(X_test, trained_params)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "    pred_label = predicted_labels[i]\n",
    "    true_label = true_labels[i]\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"Pred: {pred_label} | True: {true_label}\", color=color)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e182719",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "With just a few epochs of training, our simple neural network, built from scratch, can achieve impressive accuracy on the MNIST dataset! The visualizations show correct predictions in green and incorrect ones in red, giving us a qualitative feel for the model's performance.\n",
    "\n",
    "This notebook is the culmination of our journey, demonstrating how the abstract mathematical concepts of linear algebra (matrix multiplies), calculus (the chain rule for backpropagation), probability (softmax and cross-entropy), and optimization (gradient descent) all come together to create a powerful, learning machine."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
