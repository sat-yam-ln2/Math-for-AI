{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598e75d1",
   "metadata": {},
   "source": [
    "# ü§ñ Application 1: Linear Regression From Scratch\n",
    "\n",
    "> *\"The best way to learn is to do. The best way to build a model is to build it from scratch.\"*\n",
    "\n",
    "Welcome to the first application notebook! Here, we will tie together everything we've learned‚ÄîLinear Algebra, Calculus, and Optimization‚Äîto build one of the most fundamental machine learning models: **Linear Regression**.\n",
    "\n",
    "Instead of using a library like Scikit-learn, we will build it from scratch using only NumPy. This will give you a deep understanding of what's happening under the hood.\n",
    "\n",
    "## üéØ How the Math Comes Together\n",
    "\n",
    "- **Linear Algebra**: We'll use vector and matrix operations to make predictions efficiently. The core prediction equation, `y_hat = X @ theta`, is a direct application of matrix-vector multiplication.\n",
    "- **Calculus**: We will define a loss function (Mean Squared Error) and then calculate its gradient with respect to the model parameters. This gradient tells us how to update our parameters to improve the model.\n",
    "- **Optimization**: We will use the Gradient Descent algorithm we learned about to iteratively update our model's parameters (`theta`) and minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a3325",
   "metadata": {},
   "source": [
    "## üìö Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a0fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # y = 4 + 3x + noise\n",
    "\n",
    "print(\"ü§ñ Libraries and synthetic data loaded!\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X (Feature)')\n",
    "plt.ylabel('y (Target)')\n",
    "plt.title('Synthetic Data for Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c71bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìù Step 1: The Model and Loss Function\n",
    "\n",
    "### The Linear Model (Linear Algebra)\n",
    "Our hypothesis `h(x)` is a linear function of the input `x`:\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1 x_1 $$\n",
    "In matrix form, this becomes much cleaner:\n",
    "$$ \\hat{y} = X_b \\cdot \\theta $$\n",
    "Where:\n",
    "- `$\\hat{y}$` is the vector of predictions.\n",
    "- `$X_b$` is the design matrix with a column of ones added for the bias term `$\\theta_0$`.\n",
    "- `$\\theta$` is the parameter vector `[$\\theta_0$, $\\theta_1$]`.\n",
    "\n",
    "### The Loss Function (Calculus & Statistics)\n",
    "We'll use the Mean Squared Error (MSE), which measures the average squared difference between predictions and actual values.\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b40af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to X for the bias term (theta_0)\n",
    "# This creates the design matrix X_b\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "print(\"Original X shape:\", X.shape)\n",
    "print(\"Design Matrix X_b shape:\", X_b.shape)\n",
    "print(\"First 5 rows of X_b:\\n\", X_b[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e93a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìâ Step 2: The Gradient (Calculus)\n",
    "\n",
    "To use Gradient Descent, we need the partial derivative of the loss function `J(Œ∏)` with respect to each parameter `Œ∏_j`. The gradient vector is:\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} = \\frac{2}{m} X_b^T \\cdot (X_b \\cdot \\theta - y) $$\n",
    "\n",
    "This elegant equation gives us the gradient for all parameters at once. It's the direction of steepest ascent on the loss surface. We will move in the opposite direction to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de91c1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è Step 3: The Training (Optimization)\n",
    "\n",
    "Now we'll implement the Gradient Descent algorithm using the gradient we just defined.\n",
    "\n",
    "**The Process**:\n",
    "1. Initialize parameters `Œ∏` randomly.\n",
    "2. Repeat for a number of iterations:\n",
    "   a. Calculate the gradient of the loss function using the formula above.\n",
    "   b. Update the parameters `Œ∏` by taking a step in the opposite direction of the gradient: `Œ∏ = Œ∏ - Œ∑ * gradient`.\n",
    "3. Keep track of the loss at each step to watch it decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X_b, y, learning_rate=0.1, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model using Batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    m, n = X_b.shape\n",
    "    \n",
    "    # 1. Initialize parameters randomly\n",
    "    theta = np.random.randn(n, 1)\n",
    "    \n",
    "    # Keep track of loss and theta history\n",
    "    loss_history = []\n",
    "    theta_history = [theta]\n",
    "    \n",
    "    # 2. Repeat for n_iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        # a. Calculate the gradient\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        \n",
    "        # b. Update parameters\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "        # Calculate and store loss and theta\n",
    "        predictions = X_b.dot(theta)\n",
    "        loss = np.mean((predictions - y)**2)\n",
    "        loss_history.append(loss)\n",
    "        theta_history.append(theta)\n",
    "        \n",
    "    return theta, loss_history, theta_history\n",
    "\n",
    "# Train the model\n",
    "final_theta, loss_history, theta_history = train_linear_regression(X_b, y, learning_rate=0.1, n_iterations=100)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "print(f\"True parameters were [4, 3]\")\n",
    "print(f\"Final learned parameters (theta):\\n  Œ∏‚ÇÄ (bias): {final_theta[0][0]:.4f}\\n  Œ∏‚ÇÅ (weight): {final_theta[1][0]:.4f}\")\n",
    "print(f\"Initial Loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final Loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d406d25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Step 4: Visualization and Analysis\n",
    "\n",
    "Now let's visualize the results to see how our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95067ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(loss_history, theta_history, X, X_b, y):\n",
    "    \"\"\"\n",
    "    Visualize the training process: loss curve and regression lines.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Plot the loss curve\n",
    "    ax1.plot(loss_history)\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Mean Squared Error (Loss)')\n",
    "    ax1.set_title('Training Loss Over Time', fontsize=16, weight='bold')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Plot the regression lines at different stages\n",
    "    ax2.scatter(X, y, alpha=0.6, label='Data')\n",
    "    \n",
    "    plot_indices = [0, 10, 50, 99]\n",
    "    colors = ['r', 'orange', 'yellow', 'g']\n",
    "    \n",
    "    for i, idx in enumerate(plot_indices):\n",
    "        theta = theta_history[idx]\n",
    "        y_predict = X_b.dot(theta)\n",
    "        label = f'Iteration {idx}' if idx != 99 else f'Final Fit (Iter {idx})'\n",
    "        ax2.plot(X, y_predict, color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax2.set_xlabel('X (Feature)')\n",
    "    ax2.set_ylabel('y (Target)')\n",
    "    ax2.set_title('Learned Regression Line Over Time', fontsize=16, weight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_training(loss_history, theta_history, X, X_b, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fff5d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- **Loss Curve (Left)**: You can see the loss decreasing rapidly at the beginning and then plateauing as the model converges to the optimal parameters. This is a classic sign of successful training.\n",
    "- **Regression Line (Right)**: The plot shows how the model's prediction line starts from a random position and gradually moves to fit the data better and better with each iteration, finally settling on a line that closely matches the underlying pattern.\n",
    "\n",
    "Congratulations! You have successfully built and trained a machine learning model from scratch, seeing firsthand how linear algebra, calculus, and optimization work in harmony."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
