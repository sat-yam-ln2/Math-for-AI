{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebac056c",
   "metadata": {},
   "source": [
    "# üìà Derivatives and Gradients: The Language of Change in AI\n",
    "\n",
    "> *\"Calculus is the mathematics of change, and in AI, everything is about learning from change.\"*\n",
    "\n",
    "Welcome to the world of **calculus** - the mathematical foundation that enables AI systems to learn and optimize! Derivatives tell us how functions change, and gradients point us toward optimal solutions.\n",
    "\n",
    "## üéØ What You'll Master\n",
    "\n",
    "- **Derivatives**: Understanding rates of change geometrically and algebraically\n",
    "- **Symbolic computation**: Using SymPy for step-by-step derivative calculations\n",
    "- **Gradients**: Vector fields and optimization directions\n",
    "- **AI connections**: How derivatives power machine learning algorithms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for calculus visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, Latex\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Configure SymPy for pretty printing\n",
    "sp.init_printing(use_latex=True)\n",
    "\n",
    "print(\"üìà Calculus laboratory initialized!\")\n",
    "print(\"Ready to explore derivatives, gradients, and optimization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc72a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç Chapter 1: Understanding Derivatives\n",
    "\n",
    "## What is a Derivative?\n",
    "\n",
    "A **derivative** measures how a function changes as its input changes. Mathematically:\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**Geometric Interpretation**: The derivative is the slope of the tangent line to the function at a given point.\n",
    "\n",
    "**AI Connection**: Derivatives tell us how to adjust parameters to minimize loss functions!\n",
    "\n",
    "Let's visualize this concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bdbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_derivative_concept(func, func_derivative, x_point=2, h_values=None):\n",
    "    \"\"\"\n",
    "    Visualize the derivative as the limit of secant line slopes\n",
    "    \"\"\"\n",
    "    if h_values is None:\n",
    "        h_values = [1.0, 0.5, 0.1, 0.01]\n",
    "    \n",
    "    x = np.linspace(-1, 5, 1000)\n",
    "    y = func(x)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    \n",
    "    for i, (h, ax, color) in enumerate(zip(h_values, axes, colors)):\n",
    "        # Plot the function\n",
    "        ax.plot(x, y, 'k-', linewidth=2, label=f'f(x) = x¬≤')\n",
    "        \n",
    "        # Calculate secant line\n",
    "        x1, x2 = x_point, x_point + h\n",
    "        y1, y2 = func(x1), func(x2)\n",
    "        \n",
    "        # Secant line slope\n",
    "        slope = (y2 - y1) / (x2 - x1)\n",
    "        \n",
    "        # Plot points\n",
    "        ax.plot([x1, x2], [y1, y2], 'o', color=color, markersize=8)\n",
    "        \n",
    "        # Plot secant line\n",
    "        x_line = np.linspace(x1 - 0.5, x2 + 0.5, 100)\n",
    "        y_line = y1 + slope * (x_line - x1)\n",
    "        ax.plot(x_line, y_line, '--', color=color, linewidth=2, \n",
    "               label=f'Secant (h={h}): slope={slope:.3f}')\n",
    "        \n",
    "        # True derivative at the point\n",
    "        true_slope = func_derivative(x_point)\n",
    "        y_tangent = func(x_point) + true_slope * (x_line - x_point)\n",
    "        ax.plot(x_line, y_tangent, '-', color='black', linewidth=1, alpha=0.7,\n",
    "               label=f'Tangent: slope={true_slope:.3f}')\n",
    "        \n",
    "        ax.set_xlim(0, 4)\n",
    "        ax.set_ylim(0, 16)\n",
    "        ax.set_title(f'h = {h}', fontsize=14, weight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight the difference\n",
    "        error = abs(slope - true_slope)\n",
    "        ax.text(0.5, 14, f'Error: {error:.4f}', fontsize=12, \n",
    "               bbox=dict(boxstyle=\"round\", facecolor=color, alpha=0.3))\n",
    "    \n",
    "    plt.suptitle(f'Derivative as Limit of Secant Lines at x = {x_point}', \n",
    "                fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the limiting process\n",
    "    print(f\"üîç Derivative Approximation at x = {x_point}:\")\n",
    "    print(f\"True derivative f'({x_point}) = {func_derivative(x_point):.6f}\")\n",
    "    print(f\"\\nSecant line approximations:\")\n",
    "    for h in h_values:\n",
    "        slope = (func(x_point + h) - func(x_point)) / h\n",
    "        error = abs(slope - func_derivative(x_point))\n",
    "        print(f\"h = {h:6.2f}: slope = {slope:8.6f}, error = {error:.6f}\")\n",
    "\n",
    "# Example: f(x) = x¬≤\n",
    "def quadratic(x):\n",
    "    return x**2\n",
    "\n",
    "def quadratic_derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "print(\"üìê Visualizing Derivative Concept with f(x) = x¬≤\")\n",
    "visualize_derivative_concept(quadratic, quadratic_derivative, x_point=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57afe3",
   "metadata": {},
   "source": [
    "## üßÆ Symbolic Derivatives with SymPy\n",
    "\n",
    "Let's use SymPy to compute derivatives symbolically and see the step-by-step process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_derivatives_with_sympy():\n",
    "    \"\"\"\n",
    "    Explore various functions and their derivatives using SymPy\n",
    "    \"\"\"\n",
    "    # Define symbolic variable\n",
    "    x = sp.Symbol('x')\n",
    "    \n",
    "    # Define various functions\n",
    "    functions = {\n",
    "        'Polynomial': x**3 + 2*x**2 - 5*x + 1,\n",
    "        'Exponential': sp.exp(x),\n",
    "        'Logarithmic': sp.log(x),\n",
    "        'Trigonometric': sp.sin(x),\n",
    "        'Composite': sp.sin(x**2),\n",
    "        'Rational': 1/x,\n",
    "        'Neural Activation': 1/(1 + sp.exp(-x)),  # Sigmoid\n",
    "        'ML Loss Function': x**2 + sp.log(1 + sp.exp(-x))  # Modified logistic loss\n",
    "    }\n",
    "    \n",
    "    print(\"üßÆ Symbolic Derivative Calculations:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, func in functions.items():\n",
    "        derivative = sp.diff(func, x)\n",
    "        second_derivative = sp.diff(derivative, x)\n",
    "        \n",
    "        print(f\"\\nüìä {name}:\")\n",
    "        print(f\"   f(x)  = {func}\")\n",
    "        print(f\"   f'(x) = {derivative}\")\n",
    "        print(f\"   f''(x) = {second_derivative}\")\n",
    "        \n",
    "        # Evaluate at specific point\n",
    "        x_val = 1\n",
    "        try:\n",
    "            f_val = float(func.subs(x, x_val))\n",
    "            fp_val = float(derivative.subs(x, x_val))\n",
    "            fpp_val = float(second_derivative.subs(x, x_val))\n",
    "            \n",
    "            print(f\"   At x = {x_val}: f = {f_val:.4f}, f' = {fp_val:.4f}, f'' = {fpp_val:.4f}\")\n",
    "        except:\n",
    "            print(f\"   At x = {x_val}: Cannot evaluate (domain issues)\")\n",
    "    \n",
    "    # Demonstrate derivative rules\n",
    "    print(f\"\\n\\nüîß Derivative Rules Demonstration:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Power rule\n",
    "    n = sp.Symbol('n')\n",
    "    power_func = x**n\n",
    "    power_deriv = sp.diff(power_func, x)\n",
    "    print(f\"\\nüìê Power Rule:\")\n",
    "    print(f\"   d/dx[x^n] = {power_deriv}\")\n",
    "    \n",
    "    # Product rule\n",
    "    f = x**2\n",
    "    g = sp.sin(x)\n",
    "    product = f * g\n",
    "    product_deriv = sp.diff(product, x)\n",
    "    manual_product = sp.diff(f, x) * g + f * sp.diff(g, x)\n",
    "    print(f\"\\nüîÑ Product Rule:\")\n",
    "    print(f\"   f(x) = {f}, g(x) = {g}\")\n",
    "    print(f\"   d/dx[f¬∑g] = {product_deriv}\")\n",
    "    print(f\"   f'¬∑g + f¬∑g' = {manual_product}\")\n",
    "    print(f\"   Equal? {sp.simplify(product_deriv - manual_product) == 0}\")\n",
    "    \n",
    "    # Chain rule\n",
    "    outer = sp.sin(x)\n",
    "    inner = x**2\n",
    "    composite = sp.sin(x**2)\n",
    "    chain_deriv = sp.diff(composite, x)\n",
    "    manual_chain = sp.cos(x**2) * 2*x\n",
    "    print(f\"\\n‚õìÔ∏è  Chain Rule:\")\n",
    "    print(f\"   f(g(x)) = sin(x¬≤)\")\n",
    "    print(f\"   d/dx[f(g(x))] = {chain_deriv}\")\n",
    "    print(f\"   f'(g(x))¬∑g'(x) = {manual_chain}\")\n",
    "    print(f\"   Equal? {sp.simplify(chain_deriv - manual_chain) == 0}\")\n",
    "    \n",
    "    return functions\n",
    "\n",
    "# Explore derivatives\n",
    "function_dict = explore_derivatives_with_sympy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43440452",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Derivative Explorer\n",
    "\n",
    "Let's create an interactive tool to visualize functions and their derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_derivative_explorer(function_type='polynomial', param_a=1, param_b=1, param_c=0):\n",
    "    \"\"\"\n",
    "    Interactive exploration of functions and their derivatives\n",
    "    \"\"\"\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Define function based on type and parameters\n",
    "    if function_type == 'polynomial':\n",
    "        y = param_a * x**2 + param_b * x + param_c\n",
    "        dy_dx = 2 * param_a * x + param_b\n",
    "        title = f'f(x) = {param_a}x¬≤ + {param_b}x + {param_c}'\n",
    "        deriv_title = f\"f'(x) = {2*param_a}x + {param_b}\"\n",
    "        \n",
    "    elif function_type == 'trigonometric':\n",
    "        y = param_a * np.sin(param_b * x + param_c)\n",
    "        dy_dx = param_a * param_b * np.cos(param_b * x + param_c)\n",
    "        title = f'f(x) = {param_a}sin({param_b}x + {param_c})'\n",
    "        deriv_title = f\"f'(x) = {param_a*param_b}cos({param_b}x + {param_c})\"\n",
    "        \n",
    "    elif function_type == 'exponential':\n",
    "        y = param_a * np.exp(param_b * x) + param_c\n",
    "        dy_dx = param_a * param_b * np.exp(param_b * x)\n",
    "        title = f'f(x) = {param_a}e^({param_b}x) + {param_c}'\n",
    "        deriv_title = f\"f'(x) = {param_a*param_b}e^({param_b}x)\"\n",
    "        \n",
    "    elif function_type == 'sigmoid':\n",
    "        y = param_a / (1 + np.exp(-param_b * (x - param_c)))\n",
    "        sigmoid_term = np.exp(-param_b * (x - param_c))\n",
    "        dy_dx = param_a * param_b * sigmoid_term / (1 + sigmoid_term)**2\n",
    "        title = f'f(x) = {param_a}/(1 + e^(-{param_b}(x-{param_c})))'\n",
    "        deriv_title = \"f'(x) = sigmoid derivative\"\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot original function\n",
    "    ax1.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    ax1.set_title(title, fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('f(x)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot derivative\n",
    "    ax2.plot(x, dy_dx, 'r-', linewidth=2, label=\"f'(x)\")\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    ax2.set_title(deriv_title, fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel(\"f'(x)\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot both together\n",
    "    ax3.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    ax3.plot(x, dy_dx, 'r-', linewidth=2, label=\"f'(x)\")\n",
    "    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    ax3.set_title('Function and Derivative', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    critical_points = []\n",
    "    zero_crossings = []\n",
    "    \n",
    "    # Find approximate critical points (where derivative ‚âà 0)\n",
    "    for i in range(1, len(dy_dx)-1):\n",
    "        if abs(dy_dx[i]) < 0.1 and dy_dx[i-1] * dy_dx[i+1] < 0:\n",
    "            critical_points.append(x[i])\n",
    "    \n",
    "    # Find zero crossings of derivative\n",
    "    for i in range(len(dy_dx)-1):\n",
    "        if dy_dx[i] * dy_dx[i+1] < 0:\n",
    "            zero_crossings.append(x[i])\n",
    "    \n",
    "    print(f\"üìä Function Analysis:\")\n",
    "    print(f\"Function type: {function_type}\")\n",
    "    print(f\"Parameters: a={param_a}, b={param_b}, c={param_c}\")\n",
    "    \n",
    "    if critical_points:\n",
    "        print(f\"Critical points (f'(x) ‚âà 0): {[f'{cp:.2f}' for cp in critical_points[:3]]}\")\n",
    "    else:\n",
    "        print(f\"No critical points found in range\")\n",
    "    \n",
    "    # Behavioral analysis\n",
    "    if function_type == 'polynomial':\n",
    "        if param_a > 0:\n",
    "            print(f\"üîÑ Parabola opens upward (minimum exists)\")\n",
    "        elif param_a < 0:\n",
    "            print(f\"üîÑ Parabola opens downward (maximum exists)\")\n",
    "        else:\n",
    "            print(f\"‚û°Ô∏è Linear function (constant derivative)\")\n",
    "    \n",
    "    elif function_type == 'sigmoid':\n",
    "        print(f\"üìà Sigmoid function: S-shaped curve commonly used in neural networks\")\n",
    "        print(f\"üí° Derivative is bell-shaped, maximum at inflection point\")\n",
    "\n",
    "# Create interactive widget\n",
    "print(\"üéÆ Interactive Derivative Explorer\")\n",
    "print(\"Explore different functions and see how parameters affect their derivatives:\")\n",
    "\n",
    "interact(interactive_derivative_explorer,\n",
    "         function_type=widgets.Dropdown(\n",
    "             options=['polynomial', 'trigonometric', 'exponential', 'sigmoid'],\n",
    "             value='polynomial',\n",
    "             description='Function type:'\n",
    "         ),\n",
    "         param_a=widgets.FloatSlider(value=1, min=-3, max=3, step=0.1, description='Parameter a:'),\n",
    "         param_b=widgets.FloatSlider(value=1, min=-3, max=3, step=0.1, description='Parameter b:'),\n",
    "         param_c=widgets.FloatSlider(value=0, min=-3, max=3, step=0.1, description='Parameter c:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d81b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß≠ Chapter 2: Gradients - Vectors of Change\n",
    "\n",
    "## From Single-Variable to Multi-Variable\n",
    "\n",
    "When we have functions of multiple variables $f(x, y)$, the **gradient** generalizes the concept of derivative:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Points in direction of steepest increase\n",
    "- Magnitude indicates rate of change\n",
    "- Perpendicular to level curves\n",
    "\n",
    "**AI Connection**: Gradients tell us how to update neural network weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradients_2d():\n",
    "    \"\"\"\n",
    "    Visualize gradients for 2D functions\n",
    "    \"\"\"\n",
    "    # Define a 2D function: f(x,y) = x¬≤ + y¬≤ + xy\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2 + 0.5*x*y\n",
    "    \n",
    "    # Analytical gradient\n",
    "    def gradient(x, y):\n",
    "        df_dx = 2*x + 0.5*y\n",
    "        df_dy = 2*y + 0.5*x\n",
    "        return df_dx, df_dy\n",
    "    \n",
    "    # Create grid\n",
    "    x = np.linspace(-3, 3, 20)\n",
    "    y = np.linspace(-3, 3, 20)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Compute gradients\n",
    "    DX, DY = gradient(X, Y)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. 3D surface plot\n",
    "    ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "    ax1.set_title('3D Surface: f(x,y) = x¬≤ + y¬≤ + 0.5xy', fontsize=12, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('f(x,y)')\n",
    "    \n",
    "    # 2. Contour plot with gradient vectors\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    contour = ax2.contour(X, Y, Z, levels=15, colors='black', alpha=0.5)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Sample fewer points for gradient arrows\n",
    "    step = 3\n",
    "    ax2.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "              DX[::step, ::step], DY[::step, ::step], \n",
    "              color='red', alpha=0.7, scale=50)\n",
    "    ax2.set_title('Contours + Gradient Vectors', fontsize=12, weight='bold')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # 3. Gradient magnitude\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    magnitude = np.sqrt(DX**2 + DY**2)\n",
    "    im = ax3.imshow(magnitude, extent=[-3, 3, -3, 3], origin='lower', cmap='plasma')\n",
    "    ax3.set_title('Gradient Magnitude', fontsize=12, weight='bold')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    # 4. Level curves (detailed)\n",
    "    ax4 = fig.add_subplot(2, 3, 4)\n",
    "    contourf = ax4.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.8)\n",
    "    contour_lines = ax4.contour(X, Y, Z, levels=20, colors='white', linewidths=0.5)\n",
    "    ax4.set_title('Level Curves (Contour Plot)', fontsize=12, weight='bold')\n",
    "    ax4.set_xlabel('x')\n",
    "    ax4.set_ylabel('y')\n",
    "    plt.colorbar(contourf, ax=ax4)\n",
    "    \n",
    "    # 5. Gradient components\n",
    "    ax5 = fig.add_subplot(2, 3, 5)\n",
    "    ax5.quiver(X, Y, DX, DY, magnitude, cmap='coolwarm', scale=30)\n",
    "    ax5.set_title('Gradient Field (Color = Magnitude)', fontsize=12, weight='bold')\n",
    "    ax5.set_xlabel('x')\n",
    "    ax5.set_ylabel('y')\n",
    "    ax5.set_aspect('equal')\n",
    "    \n",
    "    # 6. Gradient descent path\n",
    "    ax6 = fig.add_subplot(2, 3, 6)\n",
    "    ax6.contour(X, Y, Z, levels=15, colors='gray', alpha=0.5)\n",
    "    \n",
    "    # Simulate gradient descent\n",
    "    start_point = [2.5, 2.0]\n",
    "    learning_rate = 0.1\n",
    "    path = [start_point]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        current = path[-1]\n",
    "        grad_x, grad_y = gradient(current[0], current[1])\n",
    "        next_point = [current[0] - learning_rate * grad_x, \n",
    "                     current[1] - learning_rate * grad_y]\n",
    "        path.append(next_point)\n",
    "    \n",
    "    path = np.array(path)\n",
    "    ax6.plot(path[:, 0], path[:, 1], 'ro-', linewidth=2, markersize=4, \n",
    "            label='Gradient Descent Path')\n",
    "    ax6.plot(path[0, 0], path[0, 1], 'go', markersize=10, label='Start')\n",
    "    ax6.plot(path[-1, 0], path[-1, 1], 'bs', markersize=10, label='End')\n",
    "    ax6.set_title('Gradient Descent Optimization', fontsize=12, weight='bold')\n",
    "    ax6.set_xlabel('x')\n",
    "    ax6.set_ylabel('y')\n",
    "    ax6.legend()\n",
    "    ax6.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze the function\n",
    "    print(f\"üß≠ Gradient Analysis for f(x,y) = x¬≤ + y¬≤ + 0.5xy:\")\n",
    "    print(f\"\\n‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy] = [2x + 0.5y, 2y + 0.5x]\")\n",
    "    \n",
    "    # Critical point (where gradient = 0)\n",
    "    print(f\"\\nüéØ Critical Point Analysis:\")\n",
    "    print(f\"Setting ‚àáf = 0:\")\n",
    "    print(f\"2x + 0.5y = 0\")\n",
    "    print(f\"2y + 0.5x = 0\")\n",
    "    print(f\"Solution: x = 0, y = 0 (minimum point)\")\n",
    "    \n",
    "    # Verify\n",
    "    critical_x, critical_y = 0, 0\n",
    "    grad_at_critical = gradient(critical_x, critical_y)\n",
    "    value_at_critical = f(critical_x, critical_y)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Verification:\")\n",
    "    print(f\"‚àáf(0,0) = {grad_at_critical}\")\n",
    "    print(f\"f(0,0) = {value_at_critical}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Gradient Descent Path:\")\n",
    "    print(f\"Started at: ({path[0, 0]:.2f}, {path[0, 1]:.2f})\")\n",
    "    print(f\"Ended at: ({path[-1, 0]:.2f}, {path[-1, 1]:.2f})\")\n",
    "    print(f\"Function value decreased from {f(path[0, 0], path[0, 1]):.3f} to {f(path[-1, 0], path[-1, 1]):.3f}\")\n",
    "\n",
    "# Visualize gradients\n",
    "visualize_gradients_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7be6dc",
   "metadata": {},
   "source": [
    "## üéØ Gradients in Machine Learning\n",
    "\n",
    "Let's see how gradients are used in a simple machine learning context - linear regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb561bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gradients_in_ml():\n",
    "    \"\"\"\n",
    "    Demonstrate how gradients are used in machine learning optimization\n",
    "    \"\"\"\n",
    "    # Generate synthetic data for linear regression\n",
    "    np.random.seed(42)\n",
    "    n_samples = 50\n",
    "    X = np.random.randn(n_samples, 1) * 2\n",
    "    true_slope = 1.5\n",
    "    true_intercept = 0.5\n",
    "    noise = np.random.randn(n_samples, 1) * 0.3\n",
    "    y = true_slope * X + true_intercept + noise\n",
    "    \n",
    "    # Define loss function (Mean Squared Error)\n",
    "    def mse_loss(w, b, X, y):\n",
    "        predictions = w * X + b\n",
    "        error = predictions - y\n",
    "        return np.mean(error**2)\n",
    "    \n",
    "    # Analytical gradients\n",
    "    def compute_gradients(w, b, X, y):\n",
    "        n = len(X)\n",
    "        predictions = w * X + b\n",
    "        error = predictions - y\n",
    "        \n",
    "        dw = (2/n) * np.sum(error * X)\n",
    "        db = (2/n) * np.sum(error)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    # Gradient descent optimization\n",
    "    def gradient_descent(X, y, learning_rate=0.01, epochs=100):\n",
    "        # Initialize parameters\n",
    "        w = np.random.randn()\n",
    "        b = np.random.randn()\n",
    "        \n",
    "        history = {'w': [w], 'b': [b], 'loss': [mse_loss(w, b, X, y)]}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Compute gradients\n",
    "            dw, db = compute_gradients(w, b, X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            # Record history\n",
    "            history['w'].append(w)\n",
    "            history['b'].append(b)\n",
    "            history['loss'].append(mse_loss(w, b, X, y))\n",
    "        \n",
    "        return w, b, history\n",
    "    \n",
    "    # Run optimization\n",
    "    final_w, final_b, history = gradient_descent(X, y, learning_rate=0.1, epochs=50)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Data and final fit\n",
    "    ax1.scatter(X, y, alpha=0.6, label='Training Data')\n",
    "    x_line = np.linspace(X.min(), X.max(), 100)\n",
    "    y_true = true_slope * x_line + true_intercept\n",
    "    y_pred = final_w * x_line + final_b\n",
    "    \n",
    "    ax1.plot(x_line, y_true, 'g--', linewidth=2, label=f'True: y = {true_slope}x + {true_intercept}')\n",
    "    ax1.plot(x_line, y_pred, 'r-', linewidth=2, \n",
    "            label=f'Learned: y = {final_w:.2f}x + {final_b:.2f}')\n",
    "    ax1.set_title('Linear Regression Result', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss surface and optimization path\n",
    "    w_range = np.linspace(0, 3, 50)\n",
    "    b_range = np.linspace(-1, 2, 50)\n",
    "    W, B = np.meshgrid(w_range, b_range)\n",
    "    \n",
    "    # Compute loss surface\n",
    "    Loss = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            Loss[i, j] = mse_loss(W[i, j], B[i, j], X, y)\n",
    "    \n",
    "    contour = ax2.contour(W, B, Loss, levels=20, colors='gray', alpha=0.5)\n",
    "    ax2.contourf(W, B, Loss, levels=20, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Plot optimization path\n",
    "    path_w = np.array(history['w'])\n",
    "    path_b = np.array(history['b'])\n",
    "    ax2.plot(path_w, path_b, 'ro-', linewidth=2, markersize=3, \n",
    "            label='Gradient Descent Path')\n",
    "    ax2.plot(path_w[0], path_b[0], 'go', markersize=10, label='Start')\n",
    "    ax2.plot(path_w[-1], path_b[-1], 'bs', markersize=10, label='End')\n",
    "    ax2.plot(true_slope, true_intercept, 'r*', markersize=15, label='True Parameters')\n",
    "    \n",
    "    ax2.set_title('Loss Surface & Optimization Path', fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('Weight (w)')\n",
    "    ax2.set_ylabel('Bias (b)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Loss curve\n",
    "    ax3.plot(history['loss'], 'b-', linewidth=2)\n",
    "    ax3.set_title('Training Loss Over Time', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Mean Squared Error')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Parameter evolution\n",
    "    epochs = range(len(history['w']))\n",
    "    ax4.plot(epochs, history['w'], 'r-', linewidth=2, label='Weight (w)')\n",
    "    ax4.plot(epochs, history['b'], 'b-', linewidth=2, label='Bias (b)')\n",
    "    ax4.axhline(y=true_slope, color='r', linestyle='--', alpha=0.7, label=f'True w = {true_slope}')\n",
    "    ax4.axhline(y=true_intercept, color='b', linestyle='--', alpha=0.7, label=f'True b = {true_intercept}')\n",
    "    ax4.set_title('Parameter Evolution', fontsize=14, weight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Parameter Value')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    final_loss = history['loss'][-1]\n",
    "    initial_loss = history['loss'][0]\n",
    "    improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "    \n",
    "    print(f\"ü§ñ Machine Learning Gradient Descent Results:\")\n",
    "    print(f\"\\nüìä Final Parameters:\")\n",
    "    print(f\"Weight (w): {final_w:.4f} (true: {true_slope})\")\n",
    "    print(f\"Bias (b): {final_b:.4f} (true: {true_intercept})\")\n",
    "    \n",
    "    print(f\"\\nüìà Training Progress:\")\n",
    "    print(f\"Initial loss: {initial_loss:.6f}\")\n",
    "    print(f\"Final loss: {final_loss:.6f}\")\n",
    "    print(f\"Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüßÆ Gradient Information:\")\n",
    "    final_dw, final_db = compute_gradients(final_w, final_b, X, y)\n",
    "    print(f\"Final gradients: dw = {final_dw:.6f}, db = {final_db:.6f}\")\n",
    "    print(f\"Gradient magnitude: {np.sqrt(final_dw**2 + final_db**2):.6f}\")\n",
    "    \n",
    "    if abs(final_dw) < 0.01 and abs(final_db) < 0.01:\n",
    "        print(f\"‚úÖ Converged! Gradients are close to zero.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Not fully converged. Consider more epochs or different learning rate.\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Demonstrate gradients in ML\n",
    "ml_history = demonstrate_gradients_in_ml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a1992",
   "metadata": {},
   "source": [
    "## üî¨ Numerical vs Analytical Gradients\n",
    "\n",
    "In practice, we often need to verify our gradient calculations. Let's compare numerical and analytical gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gradient_methods():\n",
    "    \"\"\"\n",
    "    Compare numerical and analytical gradient computation methods\n",
    "    \"\"\"\n",
    "    # Define a test function: f(x,y) = x¬≥ + y¬≤ + 2xy + sin(x)\n",
    "    def test_function(x, y):\n",
    "        return x**3 + y**2 + 2*x*y + np.sin(x)\n",
    "    \n",
    "    # Analytical gradients\n",
    "    def analytical_gradient(x, y):\n",
    "        df_dx = 3*x**2 + 2*y + np.cos(x)\n",
    "        df_dy = 2*y + 2*x\n",
    "        return df_dx, df_dy\n",
    "    \n",
    "    # Numerical gradients (finite differences)\n",
    "    def numerical_gradient(func, x, y, h=1e-8):\n",
    "        df_dx = (func(x + h, y) - func(x - h, y)) / (2 * h)\n",
    "        df_dy = (func(x, y + h) - func(x, y - h)) / (2 * h)\n",
    "        return df_dx, df_dy\n",
    "    \n",
    "    # Test points\n",
    "    test_points = [\n",
    "        (0, 0),\n",
    "        (1, 1),\n",
    "        (-1, 2),\n",
    "        (2, -1),\n",
    "        (0.5, 1.5)\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"üî¨ Numerical vs Analytical Gradient Comparison\")\n",
    "    print(f\"Function: f(x,y) = x¬≥ + y¬≤ + 2xy + sin(x)\")\n",
    "    print(f\"Analytical: ‚àáf = [3x¬≤ + 2y + cos(x), 2y + 2x]\")\n",
    "    print(f\"\\n{'Point':<12} {'Analytical':<25} {'Numerical':<25} {'Error':<15}\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    for x, y in test_points:\n",
    "        # Compute gradients\n",
    "        anal_dx, anal_dy = analytical_gradient(x, y)\n",
    "        num_dx, num_dy = numerical_gradient(test_function, x, y)\n",
    "        \n",
    "        # Compute errors\n",
    "        error_dx = abs(anal_dx - num_dx)\n",
    "        error_dy = abs(anal_dy - num_dy)\n",
    "        total_error = np.sqrt(error_dx**2 + error_dy**2)\n",
    "        \n",
    "        results.append({\n",
    "            'point': (x, y),\n",
    "            'analytical': (anal_dx, anal_dy),\n",
    "            'numerical': (num_dx, num_dy),\n",
    "            'error': total_error\n",
    "        })\n",
    "        \n",
    "        print(f\"({x:4.1f},{y:4.1f})   [{anal_dx:8.6f},{anal_dy:8.6f}]   [{num_dx:8.6f},{num_dy:8.6f}]   {total_error:10.2e}\")\n",
    "    \n",
    "    # Visualize the comparison\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Create grid for visualization\n",
    "    x = np.linspace(-2, 2, 20)\n",
    "    y = np.linspace(-2, 2, 20)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = test_function(X, Y)\n",
    "    \n",
    "    # Analytical gradients\n",
    "    DX_anal, DY_anal = analytical_gradient(X, Y)\n",
    "    \n",
    "    # 1. Function surface\n",
    "    contour1 = ax1.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.8)\n",
    "    ax1.contour(X, Y, Z, levels=20, colors='white', linewidths=0.5)\n",
    "    ax1.set_title('Function f(x,y)', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    plt.colorbar(contour1, ax=ax1)\n",
    "    \n",
    "    # 2. Analytical gradient field\n",
    "    step = 2\n",
    "    ax2.contour(X, Y, Z, levels=15, colors='gray', alpha=0.5)\n",
    "    ax2.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "              DX_anal[::step, ::step], DY_anal[::step, ::step], \n",
    "              color='red', alpha=0.7, scale=50)\n",
    "    ax2.set_title('Analytical Gradient Field', fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # 3. Numerical vs analytical comparison at test points\n",
    "    points = np.array([r['point'] for r in results])\n",
    "    errors = [r['error'] for r in results]\n",
    "    \n",
    "    scatter = ax3.scatter(points[:, 0], points[:, 1], c=errors, \n",
    "                         s=100, cmap='Reds', edgecolors='black')\n",
    "    ax3.contour(X, Y, Z, levels=10, colors='gray', alpha=0.3)\n",
    "    \n",
    "    for i, ((x, y), error) in enumerate(zip(points, errors)):\n",
    "        ax3.annotate(f'{error:.2e}', (x, y), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax3.set_title('Gradient Error at Test Points', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    plt.colorbar(scatter, ax=ax3, label='Error Magnitude')\n",
    "    \n",
    "    # 4. Error analysis\n",
    "    h_values = np.logspace(-12, -1, 50)\n",
    "    errors_vs_h = []\n",
    "    \n",
    "    test_x, test_y = 1.0, 1.0\n",
    "    anal_dx, anal_dy = analytical_gradient(test_x, test_y)\n",
    "    \n",
    "    for h in h_values:\n",
    "        num_dx, num_dy = numerical_gradient(test_function, test_x, test_y, h)\n",
    "        error = np.sqrt((anal_dx - num_dx)**2 + (anal_dy - num_dy)**2)\n",
    "        errors_vs_h.append(error)\n",
    "    \n",
    "    ax4.loglog(h_values, errors_vs_h, 'b-', linewidth=2)\n",
    "    ax4.axvline(x=1e-8, color='r', linestyle='--', alpha=0.7, label='Default h = 1e-8')\n",
    "    ax4.set_title('Numerical Error vs Step Size h', fontsize=14, weight='bold')\n",
    "    ax4.set_xlabel('Step Size (h)')\n",
    "    ax4.set_ylabel('Gradient Error')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    all_errors = [r['error'] for r in results]\n",
    "    print(f\"\\nüìà Error Analysis Summary:\")\n",
    "    print(f\"Average error: {np.mean(all_errors):.2e}\")\n",
    "    print(f\"Maximum error: {np.max(all_errors):.2e}\")\n",
    "    print(f\"Minimum error: {np.min(all_errors):.2e}\")\n",
    "    \n",
    "    if np.max(all_errors) < 1e-6:\n",
    "        print(f\"‚úÖ Excellent agreement! Numerical gradients are very accurate.\")\n",
    "    elif np.max(all_errors) < 1e-3:\n",
    "        print(f\"‚úÖ Good agreement. Numerical gradients are reasonably accurate.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Some discrepancy detected. Check implementation or use smaller h.\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    print(f\"‚Ä¢ Numerical gradients approximate analytical gradients very well\")\n",
    "    print(f\"‚Ä¢ Optimal step size h balances truncation vs round-off error\")\n",
    "    print(f\"‚Ä¢ Analytical gradients are exact and computationally efficient\")\n",
    "    print(f\"‚Ä¢ Numerical gradients useful for debugging and verification\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare gradient methods\n",
    "gradient_comparison = compare_gradient_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1d3c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## üìà Derivatives - The Foundation\n",
    "- **Rate of change**: How functions respond to input variations\n",
    "- **Geometric meaning**: Slope of tangent line at any point\n",
    "- **Limit definition**: Precise mathematical foundation\n",
    "- **Symbolic computation**: SymPy enables exact calculations\n",
    "\n",
    "## üß≠ Gradients - Multivariable Extension\n",
    "- **Vector of partial derivatives**: Direction and magnitude of steepest ascent\n",
    "- **Optimization compass**: Points toward function maximum\n",
    "- **Level curve relationship**: Always perpendicular to contours\n",
    "- **Dimensionality**: Generalizes to any number of variables\n",
    "\n",
    "## ü§ñ Machine Learning Applications\n",
    "- **Parameter optimization**: Gradients guide weight updates\n",
    "- **Loss minimization**: Following negative gradient reduces error\n",
    "- **Convergence**: Zero gradient indicates optimal solution\n",
    "- **Learning rate**: Controls step size in parameter space\n",
    "\n",
    "## üî¨ Computational Considerations\n",
    "- **Analytical vs numerical**: Exact vs approximate gradient computation\n",
    "- **Verification**: Numerical gradients validate analytical derivations\n",
    "- **Efficiency**: Analytical gradients are faster and more accurate\n",
    "- **Automatic differentiation**: Modern frameworks compute gradients automatically\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Coming Next: Chain Rule & Backpropagation\n",
    "\n",
    "Now that we understand individual derivatives and gradients, it's time to explore the **chain rule** - the mathematical foundation of neural network training:\n",
    "\n",
    "- Chain rule for composite functions\n",
    "- Backpropagation algorithm derivation\n",
    "- Neural network gradient computation\n",
    "- Computational graphs and automatic differentiation\n",
    "\n",
    "**Ready to chain your way to deep learning mastery? Let's dive into backpropagation! ‚õìÔ∏è**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
