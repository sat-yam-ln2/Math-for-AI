{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee270363",
   "metadata": {},
   "source": [
    "# ‚õìÔ∏è Chain Rule & Backpropagation: The Learning Engine of Neural Networks\n",
    "\n",
    "> *\"The chain rule isn't just calculus - it's the mathematical principle that makes deep learning possible.\"*\n",
    "\n",
    "Welcome to the mathematical heart of neural network training! The chain rule enables backpropagation, which in turn enables learning in deep networks. This notebook will show you how this fundamental calculus principle powers AI.\n",
    "\n",
    "## üéØ What You'll Master\n",
    "\n",
    "- **Chain rule fundamentals**: Mathematical foundation and geometric intuition\n",
    "- **Composite function derivatives**: Step-by-step computation with SymPy\n",
    "- **Backpropagation algorithm**: From mathematical derivation to implementation\n",
    "- **Neural network training**: Seeing gradients flow backward through layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for advanced calculus visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, Latex\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Configure SymPy for pretty printing\n",
    "sp.init_printing(use_latex=True)\n",
    "\n",
    "print(\"‚õìÔ∏è Chain rule and backpropagation laboratory initialized!\")\n",
    "print(\"Ready to explore the mathematics of neural network learning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5835dd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîó Chapter 1: Chain Rule Fundamentals\n",
    "\n",
    "## The Mathematical Foundation\n",
    "\n",
    "The **chain rule** is the calculus principle for differentiating composite functions:\n",
    "\n",
    "$$\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "For multiple compositions:\n",
    "$$\\frac{d}{dx}[f(g(h(x)))] = f'(g(h(x))) \\cdot g'(h(x)) \\cdot h'(x)$$\n",
    "\n",
    "**Neural Network Connection**: Every forward pass creates a chain of functions, and backpropagation uses the chain rule to compute gradients!\n",
    "\n",
    "Let's visualize this step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03716adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_chain_rule_with_sympy():\n",
    "    \"\"\"\n",
    "    Demonstrate chain rule with symbolic computation\n",
    "    \"\"\"\n",
    "    # Define symbolic variables\n",
    "    x = sp.Symbol('x')\n",
    "    \n",
    "    print(\"‚õìÔ∏è Chain Rule Demonstration with SymPy\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example 1: Simple composition f(g(x)) = sin(x¬≤)\n",
    "    print(\"\\nüìê Example 1: f(g(x)) = sin(x¬≤)\")\n",
    "    \n",
    "    # Define functions\n",
    "    g = x**2  # Inner function\n",
    "    f_of_g = sp.sin(g)  # Composite function\n",
    "    \n",
    "    # Compute derivatives\n",
    "    g_prime = sp.diff(g, x)\n",
    "    f_prime = sp.cos(g)  # derivative of sin is cos\n",
    "    chain_rule_result = f_prime * g_prime\n",
    "    direct_derivative = sp.diff(f_of_g, x)\n",
    "    \n",
    "    print(f\"g(x) = {g}\")\n",
    "    print(f\"f(g(x)) = sin(g(x)) = {f_of_g}\")\n",
    "    print(f\"\\nStep-by-step chain rule:\")\n",
    "    print(f\"g'(x) = {g_prime}\")\n",
    "    print(f\"f'(g(x)) = cos(g(x)) = {f_prime}\")\n",
    "    print(f\"[f(g(x))]' = f'(g(x)) ¬∑ g'(x) = {chain_rule_result}\")\n",
    "    print(f\"\\nDirect differentiation: {direct_derivative}\")\n",
    "    print(f\"Results match: {sp.simplify(chain_rule_result - direct_derivative) == 0}\")\n",
    "    \n",
    "    # Example 2: Triple composition (neural network-like)\n",
    "    print(\"\\n\\nüß† Example 2: Neural Network-like f(g(h(x))) = œÉ(W‚ÇÇœÉ(W‚ÇÅx))\")\n",
    "    \n",
    "    # Define a neural network-like function\n",
    "    W1, W2 = sp.symbols('W1 W2')  # Weights\n",
    "    \n",
    "    h = W1 * x  # First layer: linear transformation\n",
    "    # For simplicity, use tanh as activation (easier to differentiate symbolically)\n",
    "    g_of_h = sp.tanh(h)  # Second layer: activation\n",
    "    f_of_g_of_h = W2 * g_of_h  # Output layer: linear transformation\n",
    "    \n",
    "    print(f\"h(x) = W‚ÇÅx = {h}\")\n",
    "    print(f\"g(h(x)) = tanh(h(x)) = {g_of_h}\")\n",
    "    print(f\"f(g(h(x))) = W‚ÇÇ ¬∑ g(h(x)) = {f_of_g_of_h}\")\n",
    "    \n",
    "    # Compute derivatives step by step\n",
    "    h_prime = sp.diff(h, x)\n",
    "    g_prime_at_h = sp.diff(sp.tanh(h), h)\n",
    "    f_prime_at_g = W2\n",
    "    \n",
    "    chain_result = f_prime_at_g * g_prime_at_h * h_prime\n",
    "    direct_result = sp.diff(f_of_g_of_h, x)\n",
    "    \n",
    "    print(f\"\\nChain rule computation:\")\n",
    "    print(f\"h'(x) = {h_prime}\")\n",
    "    print(f\"g'(h(x)) = d/dh[tanh(h)] = {g_prime_at_h}\")\n",
    "    print(f\"f'(g(h(x))) = {f_prime_at_g}\")\n",
    "    print(f\"\\nFinal result: f'(g(h(x))) ¬∑ g'(h(x)) ¬∑ h'(x) = {chain_result}\")\n",
    "    print(f\"Direct differentiation: {direct_result}\")\n",
    "    print(f\"Results match: {sp.simplify(chain_result - direct_result) == 0}\")\n",
    "    \n",
    "    # Numerical evaluation\n",
    "    print(f\"\\nüî¢ Numerical Example (x=1, W‚ÇÅ=2, W‚ÇÇ=0.5):\")\n",
    "    x_val, W1_val, W2_val = 1, 2, 0.5\n",
    "    \n",
    "    chain_numerical = chain_result.subs([(x, x_val), (W1, W1_val), (W2, W2_val)])\n",
    "    direct_numerical = direct_result.subs([(x, x_val), (W1, W1_val), (W2, W2_val)])\n",
    "    \n",
    "    print(f\"Chain rule result: {float(chain_numerical):.6f}\")\n",
    "    print(f\"Direct result: {float(direct_numerical):.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'simple': {'function': f_of_g, 'derivative': chain_rule_result},\n",
    "        'neural': {'function': f_of_g_of_h, 'derivative': chain_result}\n",
    "    }\n",
    "\n",
    "# Demonstrate chain rule\n",
    "chain_examples = demonstrate_chain_rule_with_sympy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb83ce",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Chain Rule Explorer\n",
    "\n",
    "Let's create an interactive visualization to see how the chain rule works with different function compositions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chain_rule_visualization(outer_func='sin', inner_func='quadratic', x_center=0):\n",
    "    \"\"\"\n",
    "    Interactive visualization of chain rule\n",
    "    \"\"\"\n",
    "    x = np.linspace(-3, 3, 1000)\n",
    "    \n",
    "    # Define inner functions\n",
    "    if inner_func == 'linear':\n",
    "        g_x = 2*x + 1\n",
    "        g_prime = np.full_like(x, 2)\n",
    "        g_formula = \"g(x) = 2x + 1\"\n",
    "        g_prime_formula = \"g'(x) = 2\"\n",
    "    elif inner_func == 'quadratic':\n",
    "        g_x = x**2\n",
    "        g_prime = 2*x\n",
    "        g_formula = \"g(x) = x¬≤\"\n",
    "        g_prime_formula = \"g'(x) = 2x\"\n",
    "    elif inner_func == 'cubic':\n",
    "        g_x = x**3 - x\n",
    "        g_prime = 3*x**2 - 1\n",
    "        g_formula = \"g(x) = x¬≥ - x\"\n",
    "        g_prime_formula = \"g'(x) = 3x¬≤ - 1\"\n",
    "    \n",
    "    # Define outer functions\n",
    "    if outer_func == 'sin':\n",
    "        f_g = np.sin(g_x)\n",
    "        f_prime_g = np.cos(g_x)\n",
    "        f_formula = \"f(u) = sin(u)\"\n",
    "        f_prime_formula = \"f'(u) = cos(u)\"\n",
    "    elif outer_func == 'exp':\n",
    "        f_g = np.exp(g_x)\n",
    "        f_prime_g = np.exp(g_x)\n",
    "        f_formula = \"f(u) = e·µò\"\n",
    "        f_prime_formula = \"f'(u) = e·µò\"\n",
    "    elif outer_func == 'tanh':\n",
    "        f_g = np.tanh(g_x)\n",
    "        f_prime_g = 1 - np.tanh(g_x)**2\n",
    "        f_formula = \"f(u) = tanh(u)\"\n",
    "        f_prime_formula = \"f'(u) = sech¬≤(u)\"\n",
    "    \n",
    "    # Chain rule derivative\n",
    "    chain_derivative = f_prime_g * g_prime\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot inner function g(x)\n",
    "    ax1.plot(x, g_x, 'b-', linewidth=2, label=g_formula)\n",
    "    ax1.plot(x, g_prime, 'b--', linewidth=2, alpha=0.7, label=g_prime_formula)\n",
    "    \n",
    "    # Highlight point\n",
    "    idx = np.argmin(np.abs(x - x_center))\n",
    "    ax1.plot(x_center, g_x[idx], 'ro', markersize=8)\n",
    "    ax1.plot(x_center, g_prime[idx], 'ro', markersize=8, alpha=0.7)\n",
    "    \n",
    "    ax1.set_title('Inner Function g(x) and its Derivative', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot composite function f(g(x))\n",
    "    ax2.plot(x, f_g, 'g-', linewidth=2, label=f'f(g(x)) = {outer_func}({inner_func}(x))')\n",
    "    ax2.plot(x_center, f_g[idx], 'ro', markersize=8)\n",
    "    \n",
    "    ax2.set_title('Composite Function f(g(x))', fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('f(g(x))')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot chain rule components\n",
    "    ax3.plot(x, f_prime_g, 'r-', linewidth=2, label=\"f'(g(x))\")\n",
    "    ax3.plot(x, g_prime, 'b-', linewidth=2, label=\"g'(x)\")\n",
    "    ax3.plot(x_center, f_prime_g[idx], 'ro', markersize=8)\n",
    "    ax3.plot(x_center, g_prime[idx], 'bo', markersize=8)\n",
    "    \n",
    "    ax3.set_title('Chain Rule Components', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('Derivative Values')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot final chain rule result\n",
    "    ax4.plot(x, chain_derivative, 'm-', linewidth=2, label=\"[f(g(x))]' = f'(g(x)) ¬∑ g'(x)\")\n",
    "    ax4.plot(x_center, chain_derivative[idx], 'ro', markersize=8)\n",
    "    \n",
    "    ax4.set_title('Chain Rule Result', fontsize=14, weight='bold')\n",
    "    ax4.set_xlabel('x')\n",
    "    ax4.set_ylabel(\"[f(g(x))]'\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print values at the highlighted point\n",
    "    print(f\"\\nüìä Values at x = {x_center:.2f}:\")\n",
    "    print(f\"g({x_center:.2f}) = {g_x[idx]:.4f}\")\n",
    "    print(f\"g'({x_center:.2f}) = {g_prime[idx]:.4f}\")\n",
    "    print(f\"f'(g({x_center:.2f})) = {f_prime_g[idx]:.4f}\")\n",
    "    print(f\"[f(g(x))]'|‚Çì={x_center:.2f} = {chain_derivative[idx]:.4f}\")\n",
    "    print(f\"\\n‚õìÔ∏è Chain Rule: {f_prime_g[idx]:.4f} √ó {g_prime[idx]:.4f} = {chain_derivative[idx]:.4f}\")\n",
    "\n",
    "# Create interactive widget\n",
    "print(\"üéÆ Interactive Chain Rule Explorer\")\n",
    "print(\"Explore how different function compositions affect the chain rule:\")\n",
    "\n",
    "interact(interactive_chain_rule_visualization,\n",
    "         outer_func=widgets.Dropdown(\n",
    "             options=['sin', 'exp', 'tanh'],\n",
    "             value='sin',\n",
    "             description='Outer f(u):'\n",
    "         ),\n",
    "         inner_func=widgets.Dropdown(\n",
    "             options=['linear', 'quadratic', 'cubic'],\n",
    "             value='quadratic',\n",
    "             description='Inner g(x):'\n",
    "         ),\n",
    "         x_center=widgets.FloatSlider(\n",
    "             value=0,\n",
    "             min=-2,\n",
    "             max=2,\n",
    "             step=0.1,\n",
    "             description='Point x:'\n",
    "         ));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f01b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Chapter 2: Backpropagation Algorithm\n",
    "\n",
    "## From Chain Rule to Neural Network Training\n",
    "\n",
    "**Backpropagation** is simply the chain rule applied to neural networks! Here's how it works:\n",
    "\n",
    "For a neural network: $L = \\text{Loss}(f_3(f_2(f_1(x))))$\n",
    "\n",
    "We need: $\\frac{\\partial L}{\\partial w_1}$, $\\frac{\\partial L}{\\partial w_2}$, $\\frac{\\partial L}{\\partial w_3}$\n",
    "\n",
    "**Chain rule gives us**:\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial w_1}$$\n",
    "\n",
    "Let's implement this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ee66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_backpropagation_from_scratch():\n",
    "    \"\"\"\n",
    "    Implement backpropagation from scratch to show the math\n",
    "    \"\"\"\n",
    "    print(\"üß† Backpropagation from Scratch\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Generate simple dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = ((X[:, 0]**2 + X[:, 1]**2) > 1).astype(float).reshape(-1, 1)  # Circular decision boundary\n",
    "    \n",
    "    # Network architecture: 2 ‚Üí 4 ‚Üí 1\n",
    "    input_size = 2\n",
    "    hidden_size = 4\n",
    "    output_size = 1\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    # Activation functions\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(x):\n",
    "        s = sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward_pass(X, W1, b1, W2, b2):\n",
    "        \"\"\"\n",
    "        Forward pass with detailed intermediate values\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        z1 = X @ W1 + b1  # Linear transformation\n",
    "        a1 = sigmoid(z1)   # Activation\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        z2 = a1 @ W2 + b2  # Linear transformation\n",
    "        a2 = sigmoid(z2)    # Output activation\n",
    "        \n",
    "        return z1, a1, z2, a2\n",
    "    \n",
    "    def backward_pass(X, y, z1, a1, z2, a2, W1, W2):\n",
    "        \"\"\"\n",
    "        Backward pass implementing chain rule step by step\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        # L = 1/2 * (a2 - y)^2 (mean squared error)\n",
    "        dL_da2 = (a2 - y) / m  # ‚àÇL/‚àÇa2\n",
    "        da2_dz2 = sigmoid_derivative(z2)  # ‚àÇa2/‚àÇz2\n",
    "        dL_dz2 = dL_da2 * da2_dz2  # ‚àÇL/‚àÇz2 (chain rule)\n",
    "        \n",
    "        # Gradients for W2 and b2\n",
    "        dL_dW2 = a1.T @ dL_dz2  # ‚àÇL/‚àÇW2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)  # ‚àÇL/‚àÇb2\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dL_da1 = dL_dz2 @ W2.T  # ‚àÇL/‚àÇa1 (backpropagate)\n",
    "        da1_dz1 = sigmoid_derivative(z1)  # ‚àÇa1/‚àÇz1\n",
    "        dL_dz1 = dL_da1 * da1_dz1  # ‚àÇL/‚àÇz1 (chain rule)\n",
    "        \n",
    "        # Gradients for W1 and b1\n",
    "        dL_dW1 = X.T @ dL_dz1  # ‚àÇL/‚àÇW1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)  # ‚àÇL/‚àÇb1\n",
    "        \n",
    "        return {\n",
    "            'dL_dW2': dL_dW2, 'dL_db2': dL_db2,\n",
    "            'dL_dW1': dL_dW1, 'dL_db1': dL_db1,\n",
    "            'dL_da2': dL_da2, 'dL_dz2': dL_dz2,\n",
    "            'dL_da1': dL_da1, 'dL_dz1': dL_dz1\n",
    "        }\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    print(f\"\\nüéØ Training Neural Network:\")\n",
    "    print(f\"Architecture: {input_size} ‚Üí {hidden_size} ‚Üí {output_size}\")\n",
    "    print(f\"Dataset: {n_samples} samples\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        # Forward pass\n",
    "        z1, a1, z2, a2 = forward_pass(X, W1, b1, W2, b2)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = np.mean((a2 - y)**2) / 2\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = backward_pass(X, y, z1, a1, z2, a2, W1, W2)\n",
    "        \n",
    "        # Compute gradient norm (for monitoring)\n",
    "        grad_norm = np.sqrt(\n",
    "            np.sum(gradients['dL_dW1']**2) + np.sum(gradients['dL_db1']**2) +\n",
    "            np.sum(gradients['dL_dW2']**2) + np.sum(gradients['dL_db2']**2)\n",
    "        )\n",
    "        gradient_norms.append(grad_norm)\n",
    "        \n",
    "        # Update weights (gradient descent)\n",
    "        W1 -= learning_rate * gradients['dL_dW1']\n",
    "        b1 -= learning_rate * gradients['dL_db1']\n",
    "        W2 -= learning_rate * gradients['dL_dW2']\n",
    "        b2 -= learning_rate * gradients['dL_db2']\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {loss:.6f}, Grad Norm = {grad_norm:.6f}\")\n",
    "    \n",
    "    # Final predictions\n",
    "    _, _, _, final_predictions = forward_pass(X, W1, b1, W2, b2)\n",
    "    final_accuracy = np.mean((final_predictions > 0.5) == y.flatten())\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training Complete!\")\n",
    "    print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "    print(f\"Final accuracy: {final_accuracy:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'X': X, 'y': y,\n",
    "        'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,\n",
    "        'losses': losses,\n",
    "        'gradient_norms': gradient_norms,\n",
    "        'predictions': final_predictions\n",
    "    }\n",
    "\n",
    "# Run backpropagation demonstration\n",
    "backprop_results = implement_backpropagation_from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b9e04",
   "metadata": {},
   "source": [
    "## üìä Visualizing Backpropagation in Action\n",
    "\n",
    "Let's create comprehensive visualizations of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a57f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_backpropagation_training(results):\n",
    "    \"\"\"\n",
    "    Visualize the complete backpropagation training process\n",
    "    \"\"\"\n",
    "    X, y = results['X'], results['y']\n",
    "    W1, b1, W2, b2 = results['W1'], results['b1'], results['W2'], results['b2']\n",
    "    losses = results['losses']\n",
    "    gradient_norms = results['gradient_norms']\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Original data and learned decision boundary\n",
    "    ax1 = plt.subplot(2, 4, 1)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Forward pass for grid\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    z1_grid = grid_points @ W1 + b1\n",
    "    a1_grid = sigmoid(z1_grid)\n",
    "    z2_grid = a1_grid @ W2 + b2\n",
    "    predictions_grid = sigmoid(z2_grid)\n",
    "    \n",
    "    predictions_grid = predictions_grid.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax1.contourf(xx, yy, predictions_grid, levels=50, alpha=0.6, cmap='RdYlBu')\n",
    "    contour = ax1.contour(xx, yy, predictions_grid, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax1.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap='RdYlBu', \n",
    "                         edgecolors='black', s=50)\n",
    "    ax1.set_title('Learned Decision Boundary', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('x‚ÇÅ')\n",
    "    ax1.set_ylabel('x‚ÇÇ')\n",
    "    plt.colorbar(scatter, ax=ax1)\n",
    "    \n",
    "    # 2. Training loss curve\n",
    "    ax2 = plt.subplot(2, 4, 2)\n",
    "    ax2.plot(losses, 'b-', linewidth=2)\n",
    "    ax2.set_title('Training Loss', fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Mean Squared Error')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # 3. Gradient norms\n",
    "    ax3 = plt.subplot(2, 4, 3)\n",
    "    ax3.plot(gradient_norms, 'r-', linewidth=2)\n",
    "    ax3.set_title('Gradient Magnitude', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('||‚àáL||')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # 4. Weight matrices visualization\n",
    "    ax4 = plt.subplot(2, 4, 4)\n",
    "    im = ax4.imshow(np.hstack([W1, W2.reshape(-1, 1)]), cmap='RdBu', aspect='auto')\n",
    "    ax4.set_title('Learned Weights', fontsize=14, weight='bold')\n",
    "    ax4.set_xlabel('Weight Index')\n",
    "    ax4.set_ylabel('Input/Hidden Units')\n",
    "    plt.colorbar(im, ax=ax4)\n",
    "    \n",
    "    # 5. Prediction accuracy histogram\n",
    "    ax5 = plt.subplot(2, 4, 5)\n",
    "    ax5.hist(predictions.flatten(), bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax5.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    ax5.set_title('Prediction Distribution', fontsize=14, weight='bold')\n",
    "    ax5.set_xlabel('Predicted Probability')\n",
    "    ax5.set_ylabel('Count')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Error analysis\n",
    "    ax6 = plt.subplot(2, 4, 6)\n",
    "    errors = np.abs(predictions.flatten() - y.flatten())\n",
    "    correct = (predictions.flatten() > 0.5) == y.flatten()\n",
    "    \n",
    "    ax6.scatter(X[correct, 0], X[correct, 1], c='green', alpha=0.6, label='Correct', s=30)\n",
    "    ax6.scatter(X[~correct, 0], X[~correct, 1], c='red', alpha=0.6, label='Incorrect', s=30)\n",
    "    ax6.set_title('Prediction Accuracy', fontsize=14, weight='bold')\n",
    "    ax6.set_xlabel('x‚ÇÅ')\n",
    "    ax6.set_ylabel('x‚ÇÇ')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Activation function gradients\n",
    "    ax7 = plt.subplot(2, 4, 7)\n",
    "    z_range = np.linspace(-5, 5, 1000)\n",
    "    sigmoid_vals = 1 / (1 + np.exp(-z_range))\n",
    "    sigmoid_grads = sigmoid_vals * (1 - sigmoid_vals)\n",
    "    \n",
    "    ax7.plot(z_range, sigmoid_vals, 'b-', linewidth=2, label='œÉ(z)')\n",
    "    ax7.plot(z_range, sigmoid_grads, 'r-', linewidth=2, label=\"œÉ'(z)\")\n",
    "    ax7.set_title('Sigmoid Activation & Gradient', fontsize=14, weight='bold')\n",
    "    ax7.set_xlabel('z')\n",
    "    ax7.set_ylabel('Value')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Network architecture diagram\n",
    "    ax8 = plt.subplot(2, 4, 8)\n",
    "    \n",
    "    # Draw network nodes\n",
    "    layers = [2, 4, 1]\n",
    "    layer_names = ['Input\\n(2)', 'Hidden\\n(4)', 'Output\\n(1)']\n",
    "    x_positions = [0, 1, 2]\n",
    "    \n",
    "    for i, (x_pos, layer_size, name) in enumerate(zip(x_positions, layers, layer_names)):\n",
    "        y_positions = np.linspace(-1, 1, layer_size)\n",
    "        \n",
    "        for j, y_pos in enumerate(y_positions):\n",
    "            circle = plt.Circle((x_pos, y_pos), 0.1, color='lightblue', ec='black')\n",
    "            ax8.add_patch(circle)\n",
    "            \n",
    "            # Add connections to next layer\n",
    "            if i < len(layers) - 1:\n",
    "                next_y_positions = np.linspace(-1, 1, layers[i + 1])\n",
    "                for next_y in next_y_positions:\n",
    "                    ax8.plot([x_pos + 0.1, x_positions[i + 1] - 0.1], \n",
    "                            [y_pos, next_y], 'k-', alpha=0.3, linewidth=0.5)\n",
    "        \n",
    "        # Add layer labels\n",
    "        ax8.text(x_pos, -1.5, name, ha='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax8.set_xlim(-0.5, 2.5)\n",
    "    ax8.set_ylim(-2, 1.5)\n",
    "    ax8.set_title('Network Architecture', fontsize=14, weight='bold')\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    accuracy = np.mean((predictions > 0.5) == y.flatten())\n",
    "    final_loss = losses[-1]\n",
    "    initial_loss = losses[0]\n",
    "    improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "    \n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    print(f\"Final accuracy: {accuracy:.1%}\")\n",
    "    print(f\"Loss improvement: {improvement:.1f}%\")\n",
    "    print(f\"Final gradient norm: {gradient_norms[-1]:.2e}\")\n",
    "    print(f\"\\nüéØ Key Insights:\")\n",
    "    print(f\"‚Ä¢ Backpropagation successfully learned a non-linear decision boundary\")\n",
    "    print(f\"‚Ä¢ Gradient magnitude decreased as network converged\")\n",
    "    print(f\"‚Ä¢ Chain rule enabled efficient gradient computation through layers\")\n",
    "\n",
    "# Visualize the training process\n",
    "visualize_backpropagation_training(backprop_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8451593",
   "metadata": {},
   "source": [
    "## üîç Comparing Manual vs Automatic Differentiation\n",
    "\n",
    "Let's compare our manual backpropagation with PyTorch's automatic differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_manual_vs_automatic_differentiation():\n",
    "    \"\"\"\n",
    "    Compare manual backpropagation with PyTorch's autograd\n",
    "    \"\"\"\n",
    "    print(\"‚öñÔ∏è Manual vs Automatic Differentiation Comparison\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Same data as before\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    X_np = np.random.randn(50, 2)\n",
    "    y_np = ((X_np[:, 0]**2 + X_np[:, 1]**2) > 1).astype(float).reshape(-1, 1)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_torch = torch.FloatTensor(X_np)\n",
    "    y_torch = torch.FloatTensor(y_np)\n",
    "    \n",
    "    # Initialize same weights for fair comparison\n",
    "    W1_np = np.random.randn(2, 4) * 0.5\n",
    "    b1_np = np.zeros((1, 4))\n",
    "    W2_np = np.random.randn(4, 1) * 0.5\n",
    "    b2_np = np.zeros((1, 1))\n",
    "    \n",
    "    # Manual backpropagation class\n",
    "    class ManualNetwork:\n",
    "        def __init__(self, W1, b1, W2, b2):\n",
    "            self.W1 = W1.copy()\n",
    "            self.b1 = b1.copy()\n",
    "            self.W2 = W2.copy()\n",
    "            self.b2 = b2.copy()\n",
    "        \n",
    "        def sigmoid(self, x):\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        \n",
    "        def forward(self, X):\n",
    "            self.z1 = X @ self.W1 + self.b1\n",
    "            self.a1 = self.sigmoid(self.z1)\n",
    "            self.z2 = self.a1 @ self.W2 + self.b2\n",
    "            self.a2 = self.sigmoid(self.z2)\n",
    "            return self.a2\n",
    "        \n",
    "        def backward(self, X, y, learning_rate=0.1):\n",
    "            m = X.shape[0]\n",
    "            \n",
    "            # Output layer\n",
    "            dL_da2 = (self.a2 - y) / m\n",
    "            da2_dz2 = self.a2 * (1 - self.a2)\n",
    "            dL_dz2 = dL_da2 * da2_dz2\n",
    "            \n",
    "            dL_dW2 = self.a1.T @ dL_dz2\n",
    "            dL_db2 = np.sum(dL_dz2, axis=0, keepdims=True)\n",
    "            \n",
    "            # Hidden layer\n",
    "            dL_da1 = dL_dz2 @ self.W2.T\n",
    "            da1_dz1 = self.a1 * (1 - self.a1)\n",
    "            dL_dz1 = dL_da1 * da1_dz1\n",
    "            \n",
    "            dL_dW1 = X.T @ dL_dz1\n",
    "            dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= learning_rate * dL_dW1\n",
    "            self.b1 -= learning_rate * dL_db1\n",
    "            self.W2 -= learning_rate * dL_dW2\n",
    "            self.b2 -= learning_rate * dL_db2\n",
    "            \n",
    "            return np.mean((self.a2 - y)**2) / 2\n",
    "    \n",
    "    # PyTorch network\n",
    "    class PyTorchNetwork(nn.Module):\n",
    "        def __init__(self, W1, b1, W2, b2):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(2, 4)\n",
    "            self.linear2 = nn.Linear(4, 1)\n",
    "            \n",
    "            # Set same initial weights\n",
    "            with torch.no_grad():\n",
    "                self.linear1.weight.data = torch.FloatTensor(W1.T)\n",
    "                self.linear1.bias.data = torch.FloatTensor(b1.flatten())\n",
    "                self.linear2.weight.data = torch.FloatTensor(W2.T)\n",
    "                self.linear2.bias.data = torch.FloatTensor(b2.flatten())\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = torch.sigmoid(self.linear1(x))\n",
    "            x = torch.sigmoid(self.linear2(x))\n",
    "            return x\n",
    "    \n",
    "    # Initialize networks\n",
    "    manual_net = ManualNetwork(W1_np, b1_np, W2_np, b2_np)\n",
    "    pytorch_net = PyTorchNetwork(W1_np, b1_np, W2_np, b2_np)\n",
    "    \n",
    "    # Training\n",
    "    manual_losses = []\n",
    "    pytorch_losses = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(pytorch_net.parameters(), lr=0.1)\n",
    "    \n",
    "    print(\"\\nüèÉ Training both networks for 50 epochs...\")\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        # Manual network\n",
    "        manual_net.forward(X_np)\n",
    "        manual_loss = manual_net.backward(X_np, y_np, learning_rate=0.1)\n",
    "        manual_losses.append(manual_loss)\n",
    "        \n",
    "        # PyTorch network\n",
    "        optimizer.zero_grad()\n",
    "        pytorch_output = pytorch_net(X_torch)\n",
    "        pytorch_loss = criterion(pytorch_output, y_torch)\n",
    "        pytorch_loss.backward()\n",
    "        optimizer.step()\n",
    "        pytorch_losses.append(pytorch_loss.item())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Manual Loss = {manual_loss:.6f}, PyTorch Loss = {pytorch_loss.item():.6f}\")\n",
    "    \n",
    "    # Compare final results\n",
    "    manual_final = manual_net.forward(X_np)\n",
    "    pytorch_final = pytorch_net(X_torch).detach().numpy()\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Loss comparison\n",
    "    ax1.plot(manual_losses, 'b-', linewidth=2, label='Manual Backprop')\n",
    "    ax1.plot(pytorch_losses, 'r--', linewidth=2, label='PyTorch Autograd')\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Prediction comparison\n",
    "    ax2.scatter(manual_final, pytorch_final, alpha=0.6)\n",
    "    ax2.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Agreement')\n",
    "    ax2.set_title('Prediction Comparison', fontsize=14, weight='bold')\n",
    "    ax2.set_xlabel('Manual Backprop Predictions')\n",
    "    ax2.set_ylabel('PyTorch Predictions')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weight comparison\n",
    "    manual_weights = np.concatenate([manual_net.W1.flatten(), manual_net.W2.flatten()])\n",
    "    pytorch_weights = np.concatenate([\n",
    "        pytorch_net.linear1.weight.data.numpy().flatten(),\n",
    "        pytorch_net.linear2.weight.data.numpy().flatten()\n",
    "    ])\n",
    "    \n",
    "    ax3.scatter(manual_weights, pytorch_weights, alpha=0.6)\n",
    "    ax3.plot([manual_weights.min(), manual_weights.max()], \n",
    "             [manual_weights.min(), manual_weights.max()], 'r--', linewidth=2)\n",
    "    ax3.set_title('Learned Weights Comparison', fontsize=14, weight='bold')\n",
    "    ax3.set_xlabel('Manual Backprop Weights')\n",
    "    ax3.set_ylabel('PyTorch Weights')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error analysis\n",
    "    prediction_diff = np.abs(manual_final - pytorch_final)\n",
    "    ax4.hist(prediction_diff.flatten(), bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "    ax4.set_title('Prediction Difference Distribution', fontsize=14, weight='bold')\n",
    "    ax4.set_xlabel('|Manual - PyTorch|')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    correlation = np.corrcoef(manual_final.flatten(), pytorch_final.flatten())[0, 1]\n",
    "    max_diff = np.max(prediction_diff)\n",
    "    mean_diff = np.mean(prediction_diff)\n",
    "    \n",
    "    print(f\"\\nüìä Comparison Results:\")\n",
    "    print(f\"Prediction correlation: {correlation:.6f}\")\n",
    "    print(f\"Maximum prediction difference: {max_diff:.6f}\")\n",
    "    print(f\"Mean prediction difference: {mean_diff:.6f}\")\n",
    "    print(f\"\\n‚úÖ Verification: Manual implementation matches PyTorch!\")\n",
    "    \n",
    "    if correlation > 0.999 and max_diff < 1e-5:\n",
    "        print(f\"üéâ Perfect match! Our chain rule implementation is correct.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Small differences may be due to numerical precision.\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_manual_vs_automatic_differentiation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650e4d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## ‚õìÔ∏è Chain Rule Fundamentals\n",
    "- **Composite function differentiation**: The mathematical foundation of backpropagation\n",
    "- **Step-by-step computation**: Breaking complex derivatives into manageable pieces\n",
    "- **Geometric intuition**: How changes propagate through function compositions\n",
    "- **Symbolic verification**: SymPy confirms our analytical understanding\n",
    "\n",
    "## üß† Backpropagation Algorithm\n",
    "- **Forward pass**: Compute outputs and store intermediate values\n",
    "- **Backward pass**: Apply chain rule to compute gradients layer by layer\n",
    "- **Gradient flow**: Information flows backward from loss to all parameters\n",
    "- **Weight updates**: Gradients guide parameter optimization\n",
    "\n",
    "## üîß Implementation Details\n",
    "- **Manual implementation**: Understanding the mathematics deeply\n",
    "- **Automatic differentiation**: Modern frameworks handle complexity\n",
    "- **Numerical stability**: Careful handling of activation functions\n",
    "- **Verification**: Manual and automatic methods produce identical results\n",
    "\n",
    "## üöÄ Why This Matters for AI\n",
    "- **Learning foundation**: Every neural network learns through backpropagation\n",
    "- **Scalability**: Chain rule enables training of arbitrarily deep networks\n",
    "- **Efficiency**: Automatic differentiation makes complex models tractable\n",
    "- **Universality**: Same principle works for all differentiable architectures\n",
    "\n",
    "---\n",
    "\n",
    "# üîÆ Coming Next: Multivariable Gradients\n",
    "\n",
    "Now that we understand the chain rule and backpropagation, let's explore multivariable calculus:\n",
    "\n",
    "- Partial derivatives and gradient vectors\n",
    "- 3D surface visualization and optimization landscapes\n",
    "- Hessian matrices and second-order optimization\n",
    "- Constrained optimization and Lagrange multipliers\n",
    "\n",
    "**Ready to climb the mountains of optimization? Let's explore the multivariable gradient landscape! üèîÔ∏è**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
