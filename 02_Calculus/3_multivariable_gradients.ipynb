{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ad0f49",
   "metadata": {},
   "source": [
    "# üèîÔ∏è Multivariable Gradients: Navigating Optimization Landscapes\n",
    "\n",
    "> *\"In the mountains of machine learning, gradients are your compass, pointing toward the summit of optimal solutions.\"*\n",
    "\n",
    "Welcome to the world of **multivariable calculus** - where we explore functions of many variables and their optimization landscapes! This is where AI algorithms learn to navigate complex parameter spaces.\n",
    "\n",
    "## üéØ What You'll Master\n",
    "\n",
    "- **Partial derivatives**: Understanding how functions change in each direction\n",
    "- **Gradient vectors**: The multivariable generalization of derivatives\n",
    "- **3D surface visualization**: Seeing optimization landscapes in action\n",
    "- **Hessian matrices**: Second-order information for advanced optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for multivariable calculus visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, Latex\n",
    "from scipy.optimize import minimize\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Set up beautiful plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Configure SymPy for pretty printing\n",
    "sp.init_printing(use_latex=True)\n",
    "\n",
    "print(\"üèîÔ∏è Multivariable calculus laboratory initialized!\")\n",
    "print(\"Ready to explore optimization landscapes and gradient flows...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e496b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìê Chapter 1: Partial Derivatives and Gradients\n",
    "\n",
    "## From Single to Multiple Variables\n",
    "\n",
    "For a function $f(x, y)$, **partial derivatives** measure how the function changes when we vary one variable while holding others constant:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}$$\n",
    "\n",
    "The **gradient** combines all partial derivatives into a vector:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Points in direction of steepest increase\n",
    "- Magnitude indicates rate of change\n",
    "- Perpendicular to level curves\n",
    "\n",
    "Let's explore this with interactive examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002685a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_partial_derivatives_with_sympy():\n",
    "    \"\"\"\n",
    "    Explore partial derivatives using symbolic computation\n",
    "    \"\"\"\n",
    "    # Define symbolic variables\n",
    "    x, y = sp.symbols('x y')\n",
    "    \n",
    "    print(\"üìê Partial Derivatives with SymPy\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Example functions commonly seen in machine learning\n",
    "    functions = {\n",
    "        'Quadratic Bowl': x**2 + y**2,\n",
    "        'Saddle Point': x**2 - y**2,\n",
    "        'Rosenbrock Function': (1 - x)**2 + 100*(y - x**2)**2,\n",
    "        'Neural Network Loss': (x**2 + y**2)/2 + sp.log(1 + sp.exp(-x*y)),\n",
    "        'Himmelblau Function': (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, func in functions.items():\n",
    "        print(f\"\\nüîç {name}:\")\n",
    "        print(f\"f(x,y) = {func}\")\n",
    "        \n",
    "        # Compute partial derivatives\n",
    "        df_dx = sp.diff(func, x)\n",
    "        df_dy = sp.diff(func, y)\n",
    "        \n",
    "        print(f\"‚àÇf/‚àÇx = {df_dx}\")\n",
    "        print(f\"‚àÇf/‚àÇy = {df_dy}\")\n",
    "        \n",
    "        # Find critical points (where gradient = 0)\n",
    "        critical_points = sp.solve([df_dx, df_dy], [x, y])\n",
    "        print(f\"Critical points: {critical_points}\")\n",
    "        \n",
    "        # Compute Hessian matrix (second derivatives)\n",
    "        d2f_dx2 = sp.diff(func, x, 2)\n",
    "        d2f_dy2 = sp.diff(func, y, 2)\n",
    "        d2f_dxdy = sp.diff(func, x, y)\n",
    "        \n",
    "        hessian = sp.Matrix([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n",
    "        print(f\"Hessian matrix:\\n{hessian}\")\n",
    "        \n",
    "        # Evaluate at a test point\n",
    "        test_point = {x: 1, y: 1}\n",
    "        try:\n",
    "            grad_at_point = [float(df_dx.subs(test_point)), float(df_dy.subs(test_point))]\n",
    "            print(f\"Gradient at (1,1): [{grad_at_point[0]:.3f}, {grad_at_point[1]:.3f}]\")\n",
    "        except:\n",
    "            print(f\"Gradient at (1,1): Cannot evaluate (complex expression)\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'function': func,\n",
    "            'gradient': [df_dx, df_dy],\n",
    "            'hessian': hessian,\n",
    "            'critical_points': critical_points\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Explore partial derivatives\n",
    "symbolic_results = explore_partial_derivatives_with_sympy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2c612",
   "metadata": {},
   "source": [
    "## üéÆ Interactive 3D Surface Explorer\n",
    "\n",
    "Let's create an interactive visualization to explore different function surfaces and their gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_3d_surface_explorer(function_type='quadratic', resolution=50, show_gradients=True):\n",
    "    \"\"\"\n",
    "    Interactive 3D surface visualization with gradients\n",
    "    \"\"\"\n",
    "    # Create meshgrid\n",
    "    x = np.linspace(-3, 3, resolution)\n",
    "    y = np.linspace(-3, 3, resolution)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Define different functions\n",
    "    if function_type == 'quadratic':\n",
    "        Z = X**2 + Y**2\n",
    "        dZ_dx = 2*X\n",
    "        dZ_dy = 2*Y\n",
    "        title = \"f(x,y) = x¬≤ + y¬≤ (Convex Bowl)\"\n",
    "        \n",
    "    elif function_type == 'saddle':\n",
    "        Z = X**2 - Y**2\n",
    "        dZ_dx = 2*X\n",
    "        dZ_dy = -2*Y\n",
    "        title = \"f(x,y) = x¬≤ - y¬≤ (Saddle Point)\"\n",
    "        \n",
    "    elif function_type == 'rosenbrock':\n",
    "        Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "        dZ_dx = -2*(1 - X) - 400*X*(Y - X**2)\n",
    "        dZ_dy = 200*(Y - X**2)\n",
    "        title = \"f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤ (Rosenbrock)\"\n",
    "        \n",
    "    elif function_type == 'peaks':\n",
    "        Z = 3*(1-X)**2 * np.exp(-(X**2) - (Y+1)**2) - 10*(X/5 - X**3 - Y**5) * np.exp(-X**2-Y**2) - 1/3*np.exp(-(X+1)**2 - Y**2)\n",
    "        # Numerical gradients for complex function\n",
    "        dZ_dx = np.gradient(Z, axis=1) / (x[1] - x[0])\n",
    "        dZ_dy = np.gradient(Z, axis=0) / (y[1] - y[0])\n",
    "        title = \"f(x,y) = Peaks Function (Complex Landscape)\"\n",
    "        \n",
    "    elif function_type == 'himmelblau':\n",
    "        Z = (X**2 + Y - 11)**2 + (X + Y**2 - 7)**2\n",
    "        dZ_dx = 2*(X**2 + Y - 11)*2*X + 2*(X + Y**2 - 7)\n",
    "        dZ_dy = 2*(X**2 + Y - 11) + 2*(X + Y**2 - 7)*2*Y\n",
    "        title = \"f(x,y) = (x¬≤+y-11)¬≤ + (x+y¬≤-7)¬≤ (Himmelblau)\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 3D surface plot\n",
    "    ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, \n",
    "                           linewidth=0, antialiased=True)\n",
    "    ax1.set_title(title, fontsize=12, weight='bold')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_zlabel('f(x,y)')\n",
    "    \n",
    "    # Contour plot with gradient vectors\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    contour = ax2.contour(X, Y, Z, levels=20, colors='black', alpha=0.4, linewidths=0.5)\n",
    "    ax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    if show_gradients:\n",
    "        # Sample gradient vectors (reduce density for clarity)\n",
    "        step = max(1, resolution // 15)\n",
    "        ax2.quiver(X[::step, ::step], Y[::step, ::step], \n",
    "                  dZ_dx[::step, ::step], dZ_dy[::step, ::step], \n",
    "                  color='red', alpha=0.8, scale=50, width=0.003)\n",
    "    \n",
    "    ax2.set_title('Contour + Gradient Vectors', fontsize=12, weight='bold')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # Gradient magnitude\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    grad_magnitude = np.sqrt(dZ_dx**2 + dZ_dy**2)\n",
    "    im = ax3.imshow(grad_magnitude, extent=[-3, 3, -3, 3], origin='lower', cmap='plasma')\n",
    "    ax3.set_title('Gradient Magnitude ||‚àáf||', fontsize=12, weight='bold')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    # Partial derivative ‚àÇf/‚àÇx\n",
    "    ax4 = fig.add_subplot(2, 3, 4)\n",
    "    im1 = ax4.imshow(dZ_dx, extent=[-3, 3, -3, 3], origin='lower', cmap='RdBu')\n",
    "    ax4.set_title('Partial Derivative ‚àÇf/‚àÇx', fontsize=12, weight='bold')\n",
    "    ax4.set_xlabel('x')\n",
    "    ax4.set_ylabel('y')\n",
    "    plt.colorbar(im1, ax=ax4)\n",
    "    \n",
    "    # Partial derivative ‚àÇf/‚àÇy\n",
    "    ax5 = fig.add_subplot(2, 3, 5)\n",
    "    im2 = ax5.imshow(dZ_dy, extent=[-3, 3, -3, 3], origin='lower', cmap='RdBu')\n",
    "    ax5.set_title('Partial Derivative ‚àÇf/‚àÇy', fontsize=12, weight='bold')\n",
    "    ax5.set_xlabel('x')\n",
    "    ax5.set_ylabel('y')\n",
    "    plt.colorbar(im2, ax=ax5)\n",
    "    \n",
    "    # Cross-section analysis\n",
    "    ax6 = fig.add_subplot(2, 3, 6)\n",
    "    center_idx = resolution // 2\n",
    "    ax6.plot(x, Z[center_idx, :], 'b-', linewidth=2, label=f'f(x, y=0)')\n",
    "    ax6.plot(y, Z[:, center_idx], 'r-', linewidth=2, label=f'f(x=0, y)')\n",
    "    ax6.set_title('Cross-sections through Origin', fontsize=12, weight='bold')\n",
    "    ax6.set_xlabel('Variable Value')\n",
    "    ax6.set_ylabel('Function Value')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis\n",
    "    min_idx = np.unravel_index(np.argmin(Z), Z.shape)\n",
    "    max_idx = np.unravel_index(np.argmax(Z), Z.shape)\n",
    "    \n",
    "    print(f\"\\nüìä Function Analysis:\")\n",
    "    print(f\"Function: {title}\")\n",
    "    print(f\"Minimum value: {Z[min_idx]:.3f} at ({X[min_idx]:.2f}, {Y[min_idx]:.2f})\")\n",
    "    print(f\"Maximum value: {Z[max_idx]:.3f} at ({X[max_idx]:.2f}, {Y[max_idx]:.2f})\")\n",
    "    print(f\"Average gradient magnitude: {np.mean(grad_magnitude):.3f}\")\n",
    "    print(f\"Max gradient magnitude: {np.max(grad_magnitude):.3f}\")\n",
    "    \n",
    "    # Find approximate critical points\n",
    "    grad_threshold = np.percentile(grad_magnitude, 5)  # Bottom 5% of gradient magnitudes\n",
    "    critical_candidates = grad_magnitude < grad_threshold\n",
    "    if np.any(critical_candidates):\n",
    "        crit_indices = np.where(critical_candidates)\n",
    "        print(f\"\\nüéØ Approximate critical points (low gradient regions):\")\n",
    "        for i in range(min(5, len(crit_indices[0]))):\n",
    "            cx, cy = X[crit_indices[0][i], crit_indices[1][i]], Y[crit_indices[0][i], crit_indices[1][i]]\n",
    "            cz = Z[crit_indices[0][i], crit_indices[1][i]]\n",
    "            print(f\"  ({cx:.2f}, {cy:.2f}) ‚Üí f = {cz:.3f}\")\n",
    "\n",
    "# Create interactive widget\n",
    "print(\"üéÆ Interactive 3D Surface Explorer\")\n",
    "print(\"Explore different optimization landscapes:\")\n",
    "\n",
    "interact(interactive_3d_surface_explorer,\n",
    "         function_type=widgets.Dropdown(\n",
    "             options=['quadratic', 'saddle', 'rosenbrock', 'peaks', 'himmelblau'],\n",
    "             value='quadratic',\n",
    "             description='Function:'\n",
    "         ),\n",
    "         resolution=widgets.IntSlider(\n",
    "             value=50,\n",
    "             min=20,\n",
    "             max=100,\n",
    "             step=10,\n",
    "             description='Resolution:'\n",
    "         ),\n",
    "         show_gradients=widgets.Checkbox(\n",
    "             value=True,\n",
    "             description='Show Gradients'\n",
    "         ));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a308ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚õ∞Ô∏è Chapter 2: Gradient Descent on Complex Landscapes\n",
    "\n",
    "## Visualizing Optimization Algorithms\n",
    "\n",
    "Let's see how different optimization algorithms navigate complex multivariable landscapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff33f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_optimization_algorithms():\n",
    "    \"\"\"\n",
    "    Simulate different optimization algorithms on various landscapes\n",
    "    \"\"\"\n",
    "    print(\"‚õ∞Ô∏è Optimization Algorithm Comparison\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define test functions\n",
    "    def rosenbrock(x, y):\n",
    "        return (1 - x)**2 + 100*(y - x**2)**2\n",
    "    \n",
    "    def rosenbrock_grad(x, y):\n",
    "        dx = -2*(1 - x) - 400*x*(y - x**2)\n",
    "        dy = 200*(y - x**2)\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    def himmelblau(x, y):\n",
    "        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    \n",
    "    def himmelblau_grad(x, y):\n",
    "        dx = 2*(x**2 + y - 11)*2*x + 2*(x + y**2 - 7)\n",
    "        dy = 2*(x**2 + y - 11) + 2*(x + y**2 - 7)*2*y\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    # Optimization algorithms\n",
    "    def gradient_descent(func, grad_func, start, lr=0.01, max_iter=1000):\n",
    "        path = [start.copy()]\n",
    "        current = start.copy()\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            gradient = grad_func(current[0], current[1])\n",
    "            current = current - lr * gradient\n",
    "            path.append(current.copy())\n",
    "            \n",
    "            if np.linalg.norm(gradient) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    def momentum_gd(func, grad_func, start, lr=0.01, momentum=0.9, max_iter=1000):\n",
    "        path = [start.copy()]\n",
    "        current = start.copy()\n",
    "        velocity = np.zeros_like(start)\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            gradient = grad_func(current[0], current[1])\n",
    "            velocity = momentum * velocity - lr * gradient\n",
    "            current = current + velocity\n",
    "            path.append(current.copy())\n",
    "            \n",
    "            if np.linalg.norm(gradient) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    def adam_optimizer(func, grad_func, start, lr=0.01, beta1=0.9, beta2=0.999, \n",
    "                      epsilon=1e-8, max_iter=1000):\n",
    "        path = [start.copy()]\n",
    "        current = start.copy()\n",
    "        m = np.zeros_like(start)  # First moment\n",
    "        v = np.zeros_like(start)  # Second moment\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            gradient = grad_func(current[0], current[1])\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            m = beta1 * m + (1 - beta1) * gradient\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            v = beta2 * v + (1 - beta2) * (gradient**2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = m / (1 - beta1**(i + 1))\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = v / (1 - beta2**(i + 1))\n",
    "            \n",
    "            # Update parameters\n",
    "            current = current - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "            path.append(current.copy())\n",
    "            \n",
    "            if np.linalg.norm(gradient) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    # Test configurations\n",
    "    test_functions = {\n",
    "        'Rosenbrock': (rosenbrock, rosenbrock_grad),\n",
    "        'Himmelblau': (himmelblau, himmelblau_grad)\n",
    "    }\n",
    "    \n",
    "    optimizers = {\n",
    "        'Gradient Descent': gradient_descent,\n",
    "        'Momentum': momentum_gd,\n",
    "        'Adam': adam_optimizer\n",
    "    }\n",
    "    \n",
    "    # Run experiments\n",
    "    results = {}\n",
    "    \n",
    "    for func_name, (func, grad_func) in test_functions.items():\n",
    "        print(f\"\\nüéØ Testing on {func_name} Function:\")\n",
    "        results[func_name] = {}\n",
    "        \n",
    "        # Starting point\n",
    "        if func_name == 'Rosenbrock':\n",
    "            start = np.array([-1.5, 2.0])\n",
    "        else:  # Himmelblau\n",
    "            start = np.array([0.0, 0.0])\n",
    "        \n",
    "        for opt_name, optimizer in optimizers.items():\n",
    "            print(f\"  Running {opt_name}...\")\n",
    "            \n",
    "            if opt_name == 'Gradient Descent':\n",
    "                path = optimizer(func, grad_func, start, lr=0.001)\n",
    "            elif opt_name == 'Momentum':\n",
    "                path = optimizer(func, grad_func, start, lr=0.001, momentum=0.9)\n",
    "            else:  # Adam\n",
    "                path = optimizer(func, grad_func, start, lr=0.01)\n",
    "            \n",
    "            final_point = path[-1]\n",
    "            final_value = func(final_point[0], final_point[1])\n",
    "            iterations = len(path)\n",
    "            \n",
    "            results[func_name][opt_name] = {\n",
    "                'path': path,\n",
    "                'final_point': final_point,\n",
    "                'final_value': final_value,\n",
    "                'iterations': iterations\n",
    "            }\n",
    "            \n",
    "            print(f\"    Final point: ({final_point[0]:.4f}, {final_point[1]:.4f})\")\n",
    "            print(f\"    Final value: {final_value:.6f}\")\n",
    "            print(f\"    Iterations: {iterations}\")\n",
    "    \n",
    "    return results, test_functions\n",
    "\n",
    "# Run optimization comparison\n",
    "optimization_results, test_funcs = simulate_optimization_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8488c",
   "metadata": {},
   "source": [
    "## üìä Visualizing Optimization Paths\n",
    "\n",
    "Let's create comprehensive visualizations of how different optimizers navigate the landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimization_paths(results, test_functions):\n",
    "    \"\"\"\n",
    "    Visualize optimization paths for different algorithms\n",
    "    \"\"\"\n",
    "    colors = {'Gradient Descent': 'red', 'Momentum': 'blue', 'Adam': 'green'}\n",
    "    \n",
    "    for func_name, (func, grad_func) in test_functions.items():\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Create contour plot\n",
    "        x = np.linspace(-2, 2, 100)\n",
    "        y = np.linspace(-1, 3, 100)\n",
    "        if func_name == 'Himmelblau':\n",
    "            x = np.linspace(-5, 5, 100)\n",
    "            y = np.linspace(-5, 5, 100)\n",
    "        \n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = func(X, Y)\n",
    "        \n",
    "        # Main contour plot with all paths\n",
    "        contour = ax1.contour(X, Y, Z, levels=30, colors='gray', alpha=0.5, linewidths=0.5)\n",
    "        ax1.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.6)\n",
    "        \n",
    "        # Plot optimization paths\n",
    "        for opt_name, data in results[func_name].items():\n",
    "            path = data['path']\n",
    "            ax1.plot(path[:, 0], path[:, 1], color=colors[opt_name], \n",
    "                    linewidth=2, alpha=0.8, label=opt_name)\n",
    "            ax1.plot(path[0, 0], path[0, 1], 'ko', markersize=8, label='Start' if opt_name == 'Gradient Descent' else '')\n",
    "            ax1.plot(path[-1, 0], path[-1, 1], 'k*', markersize=12, \n",
    "                    label='End' if opt_name == 'Gradient Descent' else '')\n",
    "        \n",
    "        ax1.set_title(f'{func_name} Function - Optimization Paths', fontsize=14, weight='bold')\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Convergence curves\n",
    "        for opt_name, data in results[func_name].items():\n",
    "            path = data['path']\n",
    "            function_values = [func(point[0], point[1]) for point in path]\n",
    "            ax2.plot(function_values, color=colors[opt_name], \n",
    "                    linewidth=2, label=opt_name)\n",
    "        \n",
    "        ax2.set_title('Convergence Curves', fontsize=14, weight='bold')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Function Value')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Distance from minimum over time\n",
    "        if func_name == 'Rosenbrock':\n",
    "            true_minimum = np.array([1.0, 1.0])\n",
    "        else:  # Himmelblau has multiple minima, use one of them\n",
    "            true_minimum = np.array([3.0, 2.0])\n",
    "        \n",
    "        for opt_name, data in results[func_name].items():\n",
    "            path = data['path']\n",
    "            distances = [np.linalg.norm(point - true_minimum) for point in path]\n",
    "            ax3.plot(distances, color=colors[opt_name], \n",
    "                    linewidth=2, label=opt_name)\n",
    "        \n",
    "        ax3.set_title('Distance from Minimum', fontsize=14, weight='bold')\n",
    "        ax3.set_xlabel('Iteration')\n",
    "        ax3.set_ylabel('Distance to Minimum')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient magnitude over time\n",
    "        for opt_name, data in results[func_name].items():\n",
    "            path = data['path']\n",
    "            grad_magnitudes = [np.linalg.norm(grad_func(point[0], point[1])) for point in path]\n",
    "            ax4.plot(grad_magnitudes, color=colors[opt_name], \n",
    "                    linewidth=2, label=opt_name)\n",
    "        \n",
    "        ax4.set_title('Gradient Magnitude', fontsize=14, weight='bold')\n",
    "        ax4.set_xlabel('Iteration')\n",
    "        ax4.set_ylabel('||‚àáf||')\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìä {func_name} Function Results:\")\n",
    "        print(f\"{'Algorithm':<15} {'Iterations':<12} {'Final Value':<15} {'Distance to Min':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for opt_name, data in results[func_name].items():\n",
    "            final_distance = np.linalg.norm(data['final_point'] - true_minimum)\n",
    "            print(f\"{opt_name:<15} {data['iterations']:<12} {data['final_value']:<15.6f} {final_distance:<15.6f}\")\n",
    "\n",
    "# Visualize optimization paths\n",
    "visualize_optimization_paths(optimization_results, test_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc63d15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üßÆ Chapter 3: Hessian Matrices and Second-Order Methods\n",
    "\n",
    "## Understanding Curvature Information\n",
    "\n",
    "The **Hessian matrix** contains second-order partial derivatives:\n",
    "\n",
    "$$H = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Key Properties**:\n",
    "- Describes local curvature of the function\n",
    "- Eigenvalues determine function behavior at critical points\n",
    "- Used in Newton's method for faster convergence\n",
    "\n",
    "Let's explore Hessian matrices and their applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a874f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hessian_matrices():\n",
    "    \"\"\"\n",
    "    Analyze Hessian matrices for different functions\n",
    "    \"\"\"\n",
    "    print(\"üßÆ Hessian Matrix Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Define test functions with analytical Hessians\n",
    "    def quadratic_bowl(x, y):\n",
    "        return x**2 + y**2\n",
    "    \n",
    "    def quadratic_bowl_hessian(x, y):\n",
    "        return np.array([[2, 0], [0, 2]])\n",
    "    \n",
    "    def saddle_point(x, y):\n",
    "        return x**2 - y**2\n",
    "    \n",
    "    def saddle_point_hessian(x, y):\n",
    "        return np.array([[2, 0], [0, -2]])\n",
    "    \n",
    "    def elongated_bowl(x, y):\n",
    "        return 5*x**2 + y**2\n",
    "    \n",
    "    def elongated_bowl_hessian(x, y):\n",
    "        return np.array([[10, 0], [0, 2]])\n",
    "    \n",
    "    def rotated_ellipse(x, y):\n",
    "        return (x + y)**2 + 2*(x - y)**2\n",
    "    \n",
    "    def rotated_ellipse_hessian(x, y):\n",
    "        return np.array([[6, -2], [-2, 6]])\n",
    "    \n",
    "    test_cases = {\n",
    "        'Quadratic Bowl': (quadratic_bowl, quadratic_bowl_hessian),\n",
    "        'Saddle Point': (saddle_point, saddle_point_hessian),\n",
    "        'Elongated Bowl': (elongated_bowl, elongated_bowl_hessian),\n",
    "        'Rotated Ellipse': (rotated_ellipse, rotated_ellipse_hessian)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    for i, (name, (func, hess_func)) in enumerate(test_cases.items()):\n",
    "        # Create grid for visualization\n",
    "        x = np.linspace(-3, 3, 100)\n",
    "        y = np.linspace(-3, 3, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = func(X, Y)\n",
    "        \n",
    "        # Plot function surface\n",
    "        ax1 = axes[0, i]\n",
    "        contour = ax1.contour(X, Y, Z, levels=20, colors='black', alpha=0.4, linewidths=0.5)\n",
    "        ax1.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "        ax1.set_title(f'{name}\\n{func.__name__.replace(\"_\", \" \").title()}', fontsize=12, weight='bold')\n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_aspect('equal')\n",
    "        \n",
    "        # Analyze Hessian at origin\n",
    "        H = hess_func(0, 0)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(H)\n",
    "        determinant = np.linalg.det(H)\n",
    "        trace = np.trace(H)\n",
    "        \n",
    "        # Plot eigenvalue visualization\n",
    "        ax2 = axes[1, i]\n",
    "        \n",
    "        # Draw eigenvectors\n",
    "        scale = 0.5\n",
    "        for j, (eigenval, eigenvec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "            color = 'red' if eigenval > 0 else 'blue'\n",
    "            ax2.arrow(0, 0, scale * eigenvec[0], scale * eigenvec[1], \n",
    "                     head_width=0.1, head_length=0.1, fc=color, ec=color,\n",
    "                     linewidth=2, label=f'Œª_{j+1}={eigenval:.1f}')\n",
    "        \n",
    "        ax2.set_xlim(-1, 1)\n",
    "        ax2.set_ylim(-1, 1)\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Classify critical point\n",
    "        if determinant > 0 and trace > 0:\n",
    "            point_type = \"Local Minimum\"\n",
    "        elif determinant > 0 and trace < 0:\n",
    "            point_type = \"Local Maximum\"\n",
    "        elif determinant < 0:\n",
    "            point_type = \"Saddle Point\"\n",
    "        else:\n",
    "            point_type = \"Degenerate\"\n",
    "        \n",
    "        ax2.set_title(f'Hessian Analysis\\n{point_type}', fontsize=12, weight='bold')\n",
    "        \n",
    "        # Print analysis\n",
    "        print(f\"\\nüîç {name}:\")\n",
    "        print(f\"Hessian matrix:\\n{H}\")\n",
    "        print(f\"Eigenvalues: {eigenvalues}\")\n",
    "        print(f\"Determinant: {determinant:.3f}\")\n",
    "        print(f\"Trace: {trace:.3f}\")\n",
    "        print(f\"Classification: {point_type}\")\n",
    "        \n",
    "        if eigenvalues[0] != eigenvalues[1]:\n",
    "            condition_number = max(eigenvalues) / min(eigenvalues) if min(eigenvalues) > 0 else float('inf')\n",
    "            print(f\"Condition number: {condition_number:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "# Analyze Hessian matrices\n",
    "hessian_cases = analyze_hessian_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd53a5e",
   "metadata": {},
   "source": [
    "## üöÄ Newton's Method vs Gradient Descent\n",
    "\n",
    "Let's compare Newton's method (which uses Hessian information) with standard gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016039d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_newton_vs_gradient_descent():\n",
    "    \"\"\"\n",
    "    Compare Newton's method with gradient descent\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Newton's Method vs Gradient Descent\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define a quadratic function for fair comparison\n",
    "    def quadratic_func(x, y):\n",
    "        return 2*x**2 + 3*y**2 + x*y - 4*x - 6*y + 10\n",
    "    \n",
    "    def quadratic_grad(x, y):\n",
    "        dx = 4*x + y - 4\n",
    "        dy = 6*y + x - 6\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    def quadratic_hessian(x, y):\n",
    "        return np.array([[4, 1], [1, 6]])\n",
    "    \n",
    "    # Newton's method\n",
    "    def newton_method(func, grad_func, hess_func, start, max_iter=20):\n",
    "        path = [start.copy()]\n",
    "        current = start.copy()\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            gradient = grad_func(current[0], current[1])\n",
    "            hessian = hess_func(current[0], current[1])\n",
    "            \n",
    "            # Newton's update: x_{k+1} = x_k - H^{-1} * ‚àáf\n",
    "            try:\n",
    "                hess_inv = np.linalg.inv(hessian)\n",
    "                step = hess_inv @ gradient\n",
    "                current = current - step\n",
    "                path.append(current.copy())\n",
    "                \n",
    "                if np.linalg.norm(gradient) < 1e-10:\n",
    "                    break\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Hessian is singular, stopping Newton's method\")\n",
    "                break\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    def gradient_descent_method(func, grad_func, start, lr=0.1, max_iter=100):\n",
    "        path = [start.copy()]\n",
    "        current = start.copy()\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            gradient = grad_func(current[0], current[1])\n",
    "            current = current - lr * gradient\n",
    "            path.append(current.copy())\n",
    "            \n",
    "            if np.linalg.norm(gradient) < 1e-10:\n",
    "                break\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    # Starting points\n",
    "    start_points = [np.array([3.0, 3.0]), np.array([-2.0, 4.0]), np.array([5.0, -1.0])]\n",
    "    \n",
    "    # True minimum (solve ‚àáf = 0)\n",
    "    A = np.array([[4, 1], [1, 6]])\n",
    "    b = np.array([4, 6])\n",
    "    true_minimum = np.linalg.solve(A, b)\n",
    "    true_min_value = quadratic_func(true_minimum[0], true_minimum[1])\n",
    "    \n",
    "    print(f\"True minimum: ({true_minimum[0]:.4f}, {true_minimum[1]:.4f})\")\n",
    "    print(f\"True minimum value: {true_min_value:.6f}\")\n",
    "    \n",
    "    # Run comparisons\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Create contour plot\n",
    "    x = np.linspace(-3, 6, 100)\n",
    "    y = np.linspace(-2, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = quadratic_func(X, Y)\n",
    "    \n",
    "    for i, start in enumerate(start_points):\n",
    "        # Run optimizations\n",
    "        newton_path = newton_method(quadratic_func, quadratic_grad, quadratic_hessian, start)\n",
    "        gd_path = gradient_descent_method(quadratic_func, quadratic_grad, start, lr=0.1)\n",
    "        \n",
    "        # Plot paths\n",
    "        ax = axes[0, i]\n",
    "        contour = ax.contour(X, Y, Z, levels=20, colors='gray', alpha=0.5, linewidths=0.5)\n",
    "        ax.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "        \n",
    "        # Plot optimization paths\n",
    "        ax.plot(newton_path[:, 0], newton_path[:, 1], 'ro-', linewidth=2, \n",
    "               markersize=6, label=f'Newton ({len(newton_path)} steps)')\n",
    "        ax.plot(gd_path[:, 0], gd_path[:, 1], 'bo-', linewidth=2, \n",
    "               markersize=4, alpha=0.7, label=f'Gradient Descent ({len(gd_path)} steps)')\n",
    "        \n",
    "        # Mark start and end points\n",
    "        ax.plot(start[0], start[1], 'ks', markersize=10, label='Start')\n",
    "        ax.plot(true_minimum[0], true_minimum[1], 'k*', markersize=15, label='True Minimum')\n",
    "        \n",
    "        ax.set_title(f'Start: ({start[0]:.1f}, {start[1]:.1f})', fontsize=12, weight='bold')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Convergence analysis\n",
    "        ax2 = axes[1, i]\n",
    "        \n",
    "        # Function values over iterations\n",
    "        newton_values = [quadratic_func(point[0], point[1]) for point in newton_path]\n",
    "        gd_values = [quadratic_func(point[0], point[1]) for point in gd_path]\n",
    "        \n",
    "        ax2.plot(range(len(newton_values)), newton_values, 'ro-', linewidth=2, \n",
    "                markersize=6, label='Newton')\n",
    "        ax2.plot(range(len(gd_values)), gd_values[:len(newton_values)*3], 'bo-', linewidth=2, \n",
    "                markersize=4, alpha=0.7, label='Gradient Descent')\n",
    "        ax2.axhline(y=true_min_value, color='black', linestyle='--', \n",
    "                   alpha=0.7, label='True Minimum')\n",
    "        \n",
    "        ax2.set_title('Convergence Comparison', fontsize=12, weight='bold')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Function Value')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nStarting point {i+1}: ({start[0]:.1f}, {start[1]:.1f})\")\n",
    "        print(f\"Newton's method:\")\n",
    "        print(f\"  Final point: ({newton_path[-1, 0]:.6f}, {newton_path[-1, 1]:.6f})\")\n",
    "        print(f\"  Final value: {newton_values[-1]:.10f}\")\n",
    "        print(f\"  Iterations: {len(newton_path)}\")\n",
    "        print(f\"Gradient descent:\")\n",
    "        print(f\"  Final point: ({gd_path[-1, 0]:.6f}, {gd_path[-1, 1]:.6f})\")\n",
    "        print(f\"  Final value: {gd_values[-1]:.10f}\")\n",
    "        print(f\"  Iterations: {len(gd_path)}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ Key Insights:\")\n",
    "    print(f\"‚Ä¢ Newton's method converges quadratically (very fast)\")\n",
    "    print(f\"‚Ä¢ Gradient descent converges linearly (slower)\")\n",
    "    print(f\"‚Ä¢ Newton's method requires Hessian computation (expensive)\")\n",
    "    print(f\"‚Ä¢ For quadratic functions, Newton's method finds exact solution in one step\")\n",
    "\n",
    "# Compare Newton's method with gradient descent\n",
    "compare_newton_vs_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3aff0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## üìê Multivariable Calculus Fundamentals\n",
    "- **Partial derivatives**: Measure change in one direction while holding others constant\n",
    "- **Gradient vectors**: Combine all partial derivatives, point toward steepest ascent\n",
    "- **Level curves**: Contours where function value is constant, perpendicular to gradients\n",
    "- **Directional derivatives**: Rate of change in any specified direction\n",
    "\n",
    "## üèîÔ∏è Optimization Landscapes\n",
    "- **Convex functions**: Single global minimum, easy to optimize\n",
    "- **Non-convex functions**: Multiple local minima, challenging landscapes\n",
    "- **Saddle points**: Neither minima nor maxima, common in high dimensions\n",
    "- **Algorithm behavior**: Different optimizers navigate landscapes differently\n",
    "\n",
    "## üßÆ Second-Order Information\n",
    "- **Hessian matrices**: Capture curvature information\n",
    "- **Eigenvalue analysis**: Determines critical point classification\n",
    "- **Condition numbers**: Measure optimization difficulty\n",
    "- **Newton's method**: Uses curvature for faster convergence\n",
    "\n",
    "## üöÄ Practical Applications in AI\n",
    "- **Neural network training**: Gradient-based optimization in high dimensions\n",
    "- **Loss landscapes**: Understanding training dynamics\n",
    "- **Adaptive methods**: Adam, RMSprop use curvature approximations\n",
    "- **Convergence analysis**: Why some problems are harder than others\n",
    "\n",
    "---\n",
    "\n",
    "# üîÆ Coming Next: Probability & Statistics\n",
    "\n",
    "Having mastered calculus, we now turn to the mathematics of uncertainty:\n",
    "\n",
    "- Random variables and probability distributions\n",
    "- Bayes' theorem and probabilistic inference\n",
    "- Markov chains and stochastic processes\n",
    "- Applications to machine learning and AI\n",
    "\n",
    "**Ready to embrace uncertainty and learn the language of probabilistic AI? Let's explore the world of randomness! üé≤**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
