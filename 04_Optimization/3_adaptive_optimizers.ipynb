{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca5278",
   "metadata": {},
   "source": [
    "# ü§ñ Adaptive Optimizers: Adam and Beyond\n",
    "\n",
    "> *\"The Adam optimizer is the Swiss Army knife of deep learning. When in doubt, use Adam.\"*\n",
    "\n",
    "We've seen how SGD with Momentum can accelerate convergence. However, it uses the same learning rate for all parameters. What if some parameters need larger updates and others need smaller ones? This is the idea behind **adaptive optimizers**.\n",
    "\n",
    "This notebook introduces the most popular and effective adaptive optimizers, culminating in **Adam**, the de facto standard for training deep neural networks.\n",
    "\n",
    "## üéØ What You'll Master\n",
    "\n",
    "- **The Need for Adaptive Learning Rates**: Why a single learning rate isn't always optimal.\n",
    "- **RMSprop**: An optimizer that adapts the learning rate based on the magnitude of recent gradients.\n",
    "- **Adam (Adaptive Moment Estimation)**: The powerhouse optimizer that combines the ideas of Momentum and RMSprop.\n",
    "- **Visual Comparison**: Seeing how Adam often outperforms other optimizers in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d9621",
   "metadata": {},
   "source": [
    "## üìö Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 9)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"ü§ñ Libraries loaded for adaptive optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9829e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üí° Chapter 1: The Idea of Adaptive Learning Rates\n",
    "\n",
    "Consider a loss surface with a long, gentle slope in one direction (e.g., parameter `w`) and a steep, narrow valley in another (e.g., parameter `b`).\n",
    "\n",
    "- For `w`, we want to take **large steps** to make progress along the gentle slope.\n",
    "- For `b`, we want to take **small steps** to avoid oscillating wildly across the steep valley.\n",
    "\n",
    "A single learning rate `Œ∑` can't satisfy both needs. **Adaptive optimizers** solve this by maintaining a per-parameter learning rate that adjusts automatically during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb8a05",
   "metadata": {},
   "source": [
    "### RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop adapts the learning rate for each parameter by dividing it by a running average of the magnitudes of recent gradients for that parameter.\n",
    "\n",
    "**Intuition**: \n",
    "- If recent gradients for a parameter have been **large** (steep slope), the learning rate for that parameter is **decreased** to prevent overshooting.\n",
    "- If recent gradients have been **small** (gentle slope), the learning rate is effectively **increased** to speed up progress.\n",
    "\n",
    "### The RMSprop Update Rule\n",
    "\n",
    "1. **Update the squared gradient cache `S`**:\n",
    "   $$ S_{new} = \\gamma S_{old} + (1 - \\gamma) (\\nabla L)^2 $$\n",
    "\n",
    "2. **Update the parameters `Œ∏`**:\n",
    "   $$ \\theta_{new} = \\theta_{old} - \\frac{\\eta}{\\sqrt{S_{new}} + \\epsilon} \\nabla L(\\theta) $$\n",
    "\n",
    "Where:\n",
    "- **`Œ≥` (gamma)** is the decay rate for the cache (e.g., 0.9).\n",
    "- `(‚àáL)¬≤` is the element-wise square of the gradient.\n",
    "- **`Œµ` (epsilon)** is a small number (e.g., 1e-8) to prevent division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01352457",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üëë Chapter 2: Adam - The King of Optimizers\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** combines the best of both worlds:\n",
    "\n",
    "1. **Momentum**: It keeps a moving average of past gradients (the *first moment*), which helps accelerate convergence.\n",
    "2. **RMSprop**: It keeps a moving average of past squared gradients (the *second moment*), which provides per-parameter adaptive learning rates.\n",
    "\n",
    "Adam is generally considered robust, effective, and works well across a wide range of problems, making it the default choice for many deep learning applications.\n",
    "\n",
    "### The Adam Update Rule\n",
    "\n",
    "1. **Update biased first moment estimate (like Momentum)**:\n",
    "   $$ m_{new} = \\beta_1 m_{old} + (1 - \\beta_1) \\nabla L $$\n",
    "\n",
    "2. **Update biased second moment estimate (like RMSprop)**:\n",
    "   $$ v_{new} = \\beta_2 v_{old} + (1 - \\beta_2) (\\nabla L)^2 $$\n",
    "\n",
    "3. **Compute bias-corrected estimates**:\n",
    "   $$ \\hat{m} = \\frac{m_{new}}{1 - \\beta_1^t} $$\n",
    "   $$ \\hat{v} = \\frac{v_{new}}{1 - \\beta_2^t} $$\n",
    "   (where `t` is the timestep)\n",
    "\n",
    "4. **Update parameters `Œ∏`**:\n",
    "   $$ \\theta_{new} = \\theta_{old} - \\frac{\\eta}{\\sqrt{\\hat{v}} + \\epsilon} \\hat{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075453e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w, b):\n",
    "    \"\"\" A challenging non-convex function (Himmelblau's). \"\"\"\n",
    "    return (w**2 + b - 11)**2 + (w + b**2 - 7)**2\n",
    "\n",
    "def gradient(w, b):\n",
    "    \"\"\" Gradient for the loss function. \"\"\"\n",
    "    grad_w = 4*w*(w**2 + b - 11) + 2*(w + b**2 - 7)\n",
    "    grad_b = 2*(w**2 + b - 11) + 4*b*(w + b**2 - 7)\n",
    "    return grad_w, grad_b\n",
    "\n",
    "def run_optimizers():\n",
    "    \"\"\"\n",
    "    Run SGD, Momentum, RMSprop, and Adam to compare their paths.\n",
    "    \"\"\"\n",
    "    start_w, start_b = -0.5, 4.0\n",
    "    n_steps = 100\n",
    "    \n",
    "    optimizers = {\n",
    "        'SGD': {'path': [], 'lr': 0.01},\n",
    "        'Momentum': {'path': [], 'lr': 0.01, 'beta': 0.9, 'v_w': 0, 'v_b': 0},\n",
    "        'RMSprop': {'path': [], 'lr': 0.1, 'gamma': 0.9, 's_w': 0, 's_b': 0, 'eps': 1e-8},\n",
    "        'Adam': {'path': [], 'lr': 0.1, 'beta1': 0.9, 'beta2': 0.999, 'm_w': 0, 'm_b': 0, 'v_w': 0, 'v_b': 0, 'eps': 1e-8}\n",
    "    }\n",
    "    \n",
    "    for name, params in optimizers.items():\n",
    "        w, b = start_w, start_b\n",
    "        params['path'].append((w, b))\n",
    "        for t in range(1, n_steps + 1):\n",
    "            grad_w, grad_b = gradient(w, b)\n",
    "            \n",
    "            if name == 'SGD':\n",
    "                w -= params['lr'] * grad_w\n",
    "                b -= params['lr'] * grad_b\n",
    "            elif name == 'Momentum':\n",
    "                params['v_w'] = params['beta'] * params['v_w'] + (1 - params['beta']) * grad_w\n",
    "                params['v_b'] = params['beta'] * params['v_b'] + (1 - params['beta']) * grad_b\n",
    "                w -= params['lr'] * params['v_w']\n",
    "                b -= params['lr'] * params['v_b']\n",
    "            elif name == 'RMSprop':\n",
    "                params['s_w'] = params['gamma'] * params['s_w'] + (1 - params['gamma']) * grad_w**2\n",
    "                params['s_b'] = params['gamma'] * params['s_b'] + (1 - params['gamma']) * grad_b**2\n",
    "                w -= (params['lr'] / (np.sqrt(params['s_w']) + params['eps'])) * grad_w\n",
    "                b -= (params['lr'] / (np.sqrt(params['s_b']) + params['eps'])) * grad_b\n",
    "            elif name == 'Adam':\n",
    "                params['m_w'] = params['beta1'] * params['m_w'] + (1 - params['beta1']) * grad_w\n",
    "                params['m_b'] = params['beta1'] * params['m_b'] + (1 - params['beta1']) * grad_b\n",
    "                params['v_w'] = params['beta2'] * params['v_w'] + (1 - params['beta2']) * grad_w**2\n",
    "                params['v_b'] = params['beta2'] * params['v_b'] + (1 - params['beta2']) * grad_b**2\n",
    "                m_hat_w = params['m_w'] / (1 - params['beta1']**t)\n",
    "                m_hat_b = params['m_b'] / (1 - params['beta1']**t)\n",
    "                v_hat_w = params['v_w'] / (1 - params['beta2']**t)\n",
    "                v_hat_b = params['v_b'] / (1 - params['beta2']**t)\n",
    "                w -= (params['lr'] / (np.sqrt(v_hat_w) + params['eps'])) * m_hat_w\n",
    "                b -= (params['lr'] / (np.sqrt(v_hat_b) + params['eps'])) * m_hat_b\n",
    "            \n",
    "            params['path'].append((w, b))\n",
    "        params['path'] = np.array(params['path'])\n",
    "        \n",
    "    return optimizers\n",
    "\n",
    "def plot_optimizer_comparison(optimizers):\n",
    "    # Grid for plotting\n",
    "    w_grid = np.linspace(-5, 5, 200)\n",
    "    b_grid = np.linspace(-5, 5, 200)\n",
    "    W, B = np.meshgrid(w_grid, b_grid)\n",
    "    L = loss_function(W, B)\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.contour(W, B, L, levels=np.logspace(0, 5, 35), cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    colors = {'SGD': 'red', 'Momentum': 'orange', 'RMSprop': 'blue', 'Adam': 'black'}\n",
    "    for name, params in optimizers.items():\n",
    "        path = params['path']\n",
    "        plt.plot(path[:, 0], path[:, 1], '-o', markersize=3, color=colors[name], label=name, alpha=0.8)\n",
    "    \n",
    "    # Mark the minima\n",
    "    minima = np.array([[3, 2], [-2.805, 3.131], [-3.779, -3.283], [3.584, -1.848]])\n",
    "    plt.plot(minima[:, 0], minima[:, 1], 'y*', markersize=20, label='Local Minima', linestyle='none')\n",
    "    \n",
    "    plt.title('Comparison of Optimizer Paths on a Non-Convex Surface', fontsize=16, weight='bold')\n",
    "    plt.xlabel('Parameter w')\n",
    "    plt.ylabel('Parameter b')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.show()\n",
    "\n",
    "optimizers = run_optimizers()\n",
    "plot_optimizer_comparison(optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24931a60",
   "metadata": {},
   "source": [
    "### Analysis of the Optimizer Paths\n",
    "\n",
    "- **SGD (Red)**: Makes very slow progress. It's hampered by the small gradients in the flat regions and can't make decisive moves.\n",
    "- **Momentum (Orange)**: Does much better than SGD. It builds up velocity to shoot across the flatter regions but can still struggle and oscillate.\n",
    "- **RMSprop (Blue)**: Moves aggressively at the start because the adaptive learning rate helps it navigate the different curvatures. It finds a minimum efficiently.\n",
    "- **Adam (Black)**: The clear winner. It combines the aggressive, adaptive steps of RMSprop with the path-smoothing, accelerating properties of Momentum. It takes the most direct and rapid path to the nearest minimum.\n",
    "\n",
    "This visualization clearly shows why Adam is the preferred optimizer for most complex, high-dimensional problems found in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2373d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## üß† The Big Idea\n",
    "- **Adaptive Learning Rates**: The core innovation is to maintain a per-parameter learning rate, allowing the optimizer to adapt to the specific geometry of the loss surface for each parameter.\n",
    "\n",
    "## üõ†Ô∏è The Building Blocks\n",
    "- **RMSprop**: Adapts learning rates by dividing by a moving average of squared gradients. This helps balance progress on flat vs. steep directions.\n",
    "- **Momentum**: Accelerates progress by adding a moving average of past gradients to the current update, smoothing the path.\n",
    "\n",
    "## üëë Adam: The Best of Both Worlds\n",
    "- **Adam** combines the adaptive learning rate mechanism of RMSprop with the velocity-building mechanism of Momentum.\n",
    "- It is robust, efficient, and generally requires less manual tuning of the learning rate than other optimizers, making it an excellent default choice.\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ What's Next?\n",
    "\n",
    "This concludes our journey through the fundamentals of Optimization! We have seen how to navigate complex loss surfaces to find the best parameters for our models. We've built up from the basic idea of Gradient Descent to the sophisticated, state-of-the-art Adam optimizer.\n",
    "\n",
    "The final section of this course, **Applications in Machine Learning**, will tie everything together. We'll see how Linear Algebra, Calculus, Probability, and Optimization all converge to build and train real machine learning models.\n",
    "\n",
    "**Ready to see it all come together? Let's build some models! ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
